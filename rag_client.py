import os
import re
import shutil # Import shutil for file operations
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
#åŠ å…¥è®°å¿†åŠŸèƒ½
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import Document
from sentence_transformers import SentenceTransformer, util
import torch
from langchain.embeddings.base import Embeddings
from transformers import AutoTokenizer, AutoModel
import numpy as np
from typing import List
from chromadb import Client
from chromadb.config import Settings
import json
import hashlib
from datetime import datetime
from langchain.retrievers import EnsembleRetriever
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
"""
ä¸Šä¼ å’Œç®¡ç†PDF/DOCXæ–‡æ¡£çŸ¥è¯†åº“
åŸºäºæ–‡æ¡£å†…å®¹æå‡ºè‡ªç„¶è¯­è¨€é—®é¢˜
ç³»ç»Ÿè‡ªåŠ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆç­”æ¡ˆ
ä½¿ç”¨Qwen 3-8Bå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå›ç­”ç”Ÿæˆ
æ”¯æŒæµå¼è¾“å‡ºå’ŒChatGPTé£æ ¼ç•Œé¢
"""
#å·¥ä½œæµç¨‹ æ–‡æ¡£åŠ è½½ â†’ æ–‡æœ¬åˆ†å‰² â†’ å‘é‡åµŒå…¥ â†’ å‘é‡æ•°æ®åº“å­˜å‚¨ â†’ é—®é¢˜æ£€ç´¢ â†’ RAGå›ç­”ç”Ÿæˆ
# --- Configuration ---
DOCUMENTS_DIR = "./documents"  # Modify to your document directory
PERSIST_DIR = "./chroma_db"     # Vector database storage directory å‘é‡æ•°æ®åº“ å­˜å‚¨æ•°æ®çš„ï¼ˆä¸ªäººç†è§£ï¼‰
CHAT_HISTORY_PERSIST_DIR = "./chat_history_db"  # å¯¹è¯å†å²å‘é‡æ•°æ®åº“å­˜å‚¨ç›®å½•
EMBEDDING_MODEL_PATH = "model/bge-m3" # åµŒå…¥æ¨¡å‹è·¯å¾„ å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡    æ³¨æ„è¿™ä¸ªæ¨¡å‹å‡ºæ¥çš„å‘é‡éƒ½æ˜¯å½’ä¸€åŒ–çš„
EMBEDDING_DEVICE = "cuda:1" # Or 'cpu' åµŒå…¥æ¨¡å‹è®¾å¤‡
# VLLM Server details (using OpenAI compatible endpoint)
VLLM_BASE_URL = "http://localhost:7861/v1"  # ä½¿ç”¨æ­£ç¡®çš„ç«¯å£ 7861
#VLLM_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"  #è°ƒç”¨å¤–éƒ¨API
VLLM_API_KEY = "dummy-key" # Required by ChatOpenAI, but VLLM server doesn't usually check it 
#VLLM_API_KEY = "sk-dcc523ef2e27471895aef2bdd7a2efc4" # Required by ChatOpenAI, but VLLM server doesn't usually check it 
VLLM_MODEL_NAME = "/mnt/jrwbxx/LLM/model/qwen3-1.7b"  # ä¿®æ­£æ¨¡å‹è·¯å¾„
#VLLM_MODEL_NAME = "qwen3-1.7b"  #ä½¿ç”¨å¤–éƒ¨çš„API

# æ£€ç´¢å‚æ•° æ£€ç´¢çš„é…ç½® è§†æƒ…å†µæ”¹
CHUNK_SIZE = 512 # Adjusted for bge-m3, which can handle more context  æ–‡æœ¬å—å¤§å°
CHUNK_OVERLAP = 50  # Adjusted overlap (approx 20% of CHUNK_SIZE)  æ–‡æœ¬å—é‡å å¤§å° è¿™ä¸ªçš„ç›®çš„æˆ‘ä¸ªäººè§‰å¾—æ˜¯ç¡®ä¿æ¯ä¸ªå—ä¹‹é—´æœ‰è”ç³»
SEARCH_K = 10 # Retrieve more chunks to increase chances of finding specific sentences  æ£€ç´¢åˆ°çš„ç»“æœçš„æ•°é‡
CHAT_HISTORY_SEARCH_K = 2  # å¯¹è¯å†å²æ£€ç´¢æ•°é‡
# --- End Configuration ---

# Global variables
rag_chain = None
vector_db = None
chat_history_vector_db = None  # æ–°å¢ï¼šå¯¹è¯å†å²å‘é‡æ•°æ®åº“
embeddings = None
llm = None
# ç§»é™¤å†…å­˜å­˜å‚¨ï¼Œæ”¹ä¸ºä½¿ç”¨å‘é‡åº“å­˜å‚¨å¯¹è¯å†å²
user_facts = []

# æ–‡æ¡£ç´¢å¼•è·Ÿè¸ª
DOCUMENT_INDEX_FILE = "./document_index.json"  # å­˜å‚¨å·²å¤„ç†æ–‡æ¡£ä¿¡æ¯çš„æ–‡ä»¶
document_index = {}  # å†…å­˜ä¸­çš„æ–‡æ¡£ç´¢å¼• {file_path: {"hash": file_hash, "mtime": mtime, "size": size}}

def load_document_index():
    """åŠ è½½æ–‡æ¡£ç´¢å¼•æ–‡ä»¶"""
    global document_index
    try:
        if os.path.exists(DOCUMENT_INDEX_FILE):
            with open(DOCUMENT_INDEX_FILE, 'r', encoding='utf-8') as f:
                document_index = json.load(f) #å°†jsonæ–‡ä»¶è¯»è¿›æ¥ï¼Œå˜æˆpythonå¯¹è±¡  è¯»å–fä¸­å…¨éƒ¨å†…å®¹ å¿…é¡»jsonæ ¼å¼ å°†jsonå­—ç¬¦ä¸²è§£ææˆpythonå¯¹è±¡  
            print(f"å·²åŠ è½½æ–‡æ¡£ç´¢å¼•ï¼ŒåŒ…å« {len(document_index)} ä¸ªæ–‡ä»¶è®°å½•")
        else:
            document_index = {}
            print("æ–‡æ¡£ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„ç´¢å¼•")
    except Exception as e:
        print(f"åŠ è½½æ–‡æ¡£ç´¢å¼•æ—¶å‡ºé”™: {e}")
        document_index = {}

def save_document_index():
    """ä¿å­˜æ–‡æ¡£ç´¢å¼•åˆ°æ–‡ä»¶"""
    try:
        with open(DOCUMENT_INDEX_FILE, 'w', encoding='utf-8') as f:
            json.dump(document_index, f, ensure_ascii=False, indent=2)
        print(f"æ–‡æ¡£ç´¢å¼•å·²ä¿å­˜ï¼ŒåŒ…å« {len(document_index)} ä¸ªæ–‡ä»¶è®°å½•")
    except Exception as e:
        print(f"ä¿å­˜æ–‡æ¡£ç´¢å¼•æ—¶å‡ºé”™: {e}")

def get_file_info(file_path):
    """è·å–æ–‡ä»¶ä¿¡æ¯ï¼šå“ˆå¸Œå€¼ã€ä¿®æ”¹æ—¶é—´ã€æ–‡ä»¶å¤§å°"""
    try:
        stat = os.stat(file_path)
        mtime = stat.st_mtime
        size = stat.st_size
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆä½¿ç”¨æ–‡ä»¶çš„å‰1MBå’Œå1MBæ¥å¿«é€Ÿè®¡ç®—ï¼‰  md5æ˜¯å¸¸è§çš„hashè®¡ç®—æ–¹æ³•  èƒ½æ£€æµ‹æ–‡æ¡£å†…å®¹å˜åŒ–  å¯¹äº2mbä»¥ä¸‹æ–‡ä»¶ è¯»å–å‰1mbçš„å†…å®¹ å¯¹äº2mbä»¥ä¸Šæ–‡ä»¶ è¯»å–å‰1mbå’Œå1mbçš„å†…å®¹
        hash_md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            # è¯»å–å‰1MB
            data = f.read(1024 * 1024)
            hash_md5.update(data)
            
            # å¦‚æœæ–‡ä»¶å¤§äº2MBï¼Œè¯»å–å1MB
            if size > 2 * 1024 * 1024:
                f.seek(-1024 * 1024, 2)  # ä»æ–‡ä»¶æœ«å°¾å‘å‰1MB
                data = f.read(1024 * 1024)
                hash_md5.update(data)
        
        return {
            "hash": hash_md5.hexdigest(),
            "mtime": mtime,
            "size": size
        }
    except Exception as e:
        print(f"è·å–æ–‡ä»¶ä¿¡æ¯æ—¶å‡ºé”™ {file_path}: {e}")
        return None

def is_file_unchanged(file_path):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœªå‘ç”Ÿå˜åŒ–"""
    if file_path not in document_index:
        return False
    
    current_info = get_file_info(file_path)
    if current_info is None:
        return False
    
    stored_info = document_index[file_path]
    return (current_info["hash"] == stored_info["hash"] and    #åˆ†åˆ«æŸ¥çœ‹hash ä¿®æ”¹æ—¶é—´ æ–‡ä»¶å¤§å° æ˜¯å¦ä¸€è‡´ ä¸€è‡´çš„æƒ…å†µå°±è¿”å›True 
            current_info["mtime"] == stored_info["mtime"] and 
            current_info["size"] == stored_info["size"])

def update_document_index(file_path):
    """æ›´æ–°æ–‡æ¡£ç´¢å¼•ä¸­çš„æ–‡ä»¶ä¿¡æ¯"""
    file_info = get_file_info(file_path)
    if file_info:
        document_index[file_path] = file_info
        print(f"å·²æ›´æ–°æ–‡æ¡£ç´¢å¼•: {os.path.basename(file_path)}")

def rewrite_question_if_needed(current_question: str, previous_question: str):
    """
    å¤§æ¨¡å‹åˆ¤æ–­å½“å‰é—®é¢˜æ˜¯å¦éœ€è¦é‡å†™ï¼Œå¦‚æœéœ€è¦ï¼Œåˆ™ä½¿ç”¨å¤§æ¨¡å‹é‡å†™ä¸€ä¸ªæ›´åˆç†çš„é—®é¢˜ï¼Œå¦åˆ™è¿”å›åŸå§‹é—®é¢˜ã€‚
    """

    rewrite_prompt = f"""è¯·æ ¹æ®ä¸Šä¸‹æ–‡é‡å†™ä»¥ä¸‹é—®é¢˜ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°å’Œå®Œæ•´ã€‚

å‰ä¸€ä¸ªé—®é¢˜ï¼š{previous_question}
å½“å‰é—®é¢˜ï¼š{current_question}

è¯·é‡å†™å½“å‰é—®é¢˜ï¼Œä½¿å…¶ï¼š
1. å¦‚æœå½“å‰é—®é¢˜ä¸æ˜ç¡®ï¼Œä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„é—®é¢˜ï¼Œå°±ç»“åˆå‰ä¸€ä¸ªé—®é¢˜ï¼Œä½¿é—®é¢˜æ›´åŠ æ˜ç¡®å’Œå…·ä½“
2. é‡å†™åçš„é—®é¢˜ä¸éœ€è¦ç»“åˆå…¶ä»–é—®é¢˜ä¹ŸçŸ¥é“åœ¨é—®ä»€ä¹ˆ
3. åªéœ€è¦æ”¹å†™é—®é¢˜ï¼Œä¸éœ€è¦è§£é‡Šè¯´æ˜
4. å¦‚æœå½“å‰é—®é¢˜å’Œå‰ä¸€ä¸ªé—®é¢˜æ²¡æœ‰ç›´æ¥å…³ç³»ï¼Œåˆ™è¿”å›å½“å‰é—®é¢˜
5. å¦‚æœå½“å‰é—®é¢˜å·²ç»æ˜¯ä¸€ä¸ªå®Œæ•´çš„é—®é¢˜ï¼Œé‚£ä¹ˆä¸éœ€è¦æ”¹å†™ï¼Œç›´æ¥è¿”å›å½“å‰é—®é¢˜
é‡å†™åçš„é—®é¢˜ï¼š"""
        
    try:
        # ä½¿ç”¨RAGæ ¸å¿ƒä¸­çš„LLMæ¥é‡å†™é—®é¢˜
        rewritten_response = ""
        for chunk in llm.invoke(rewrite_prompt):
            rewritten_response += chunk.content
        
        # æ¸…ç†å“åº”ï¼Œåªä¿ç•™é‡å†™çš„é—®é¢˜éƒ¨åˆ†
        rewritten_question = rewritten_response.strip()

        print("è¿™æ˜¯å¤§æ¨¡å‹é‡å†™çš„ç»“æœ",rewritten_question)
        
        # å¦‚æœå“åº”å¤ªé•¿ï¼Œå¯èƒ½åŒ…å«äº†é¢å¤–çš„è§£é‡Šï¼Œå°è¯•æå–é—®é¢˜éƒ¨åˆ†
        if len(rewritten_question) > len(current_question) * 3:
            # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªé—®å·æˆ–å¥å·ä½œä¸ºé—®é¢˜çš„ç»“æŸ
            for i in range(len(rewritten_question) - 1, -1, -1):
                if rewritten_question[i] in ['ï¼Ÿ', '?', 'ã€‚', '.']:
                    rewritten_question = rewritten_question[:i+1]
                    break
        
        print(f"é—®é¢˜å·²ç”±å¤§æ¨¡å‹é‡å†™: {rewritten_question}")
        return rewritten_question
        
    except Exception as e:
        print(f"å¤§æ¨¡å‹é‡å†™é—®é¢˜å¤±è´¥: {e}")
        # å¦‚æœå¤§æ¨¡å‹é‡å†™å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•çš„é‡å†™æ–¹å¼
        rewritten = f"å…³äº\"{previous_question}\"ï¼Œ{current_question}"
        return rewritten

    
# 1. å®šä¹‰æ–‡æ¡£åŠ è½½å‡½æ•°ï¼Œæ”¯æŒPDFå’ŒWord ä»¥åŠè¿”å›æ–‡æ¡£å†…å®¹åˆ—è¡¨
def load_documents(directory_path, incremental=True):
    """
    åŠ è½½æ–‡æ¡£ï¼Œæ”¯æŒå¢é‡æ›´æ–°
    
    å‚æ•°:
        directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
        incremental: æ˜¯å¦å¯ç”¨å¢é‡æ›´æ–°æ¨¡å¼
    """
    documents = []
    new_files = []
    skipped_files = []

    for file in os.listdir(directory_path):
        file_path = os.path.join(directory_path, file)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†ä¸”æœªä¿®æ”¹
        if incremental and is_file_unchanged(file_path):
            skipped_files.append(file)
            continue
            
        try:
            if file.endswith('.pdf'): #å°†pdfæ–‡ä»¶è§£æä¸ºå¤šä¸ªç‰‡æ®µï¼Œç„¶åå°†è¿™äº›ç‰‡æ®µæ”¾å…¥å¤§çš„documentä¸­
                loader = PyPDFLoader(file_path) #è¿™æ˜¯ä¸€ä¸ªåŠ è½½å™¨ï¼Œè¯»å–pdfæ–‡ä»¶ï¼Œè½¬åŒ–ä¸ºæ–‡æ¡£ç‰‡æ®µï¼ˆdocument chunksï¼‰
                file_documents = loader.load() #loader.load()è¿”å›ä¸€ä¸ªåˆ—è¡¨ éƒ½æ˜¯documentå¯¹è±¡
                documents.extend(file_documents) #ç„¶åé€šè¿‡extendå°†æ‰€æœ‰çš„å¯¹è±¡æ”¾åœ¨documentä¸­
                new_files.append(file)
                # æ›´æ–°æ–‡æ¡£ç´¢å¼•
                if incremental:
                    update_document_index(file_path)
            elif file.endswith('.docx') or file.endswith('.doc'):
                loader = Docx2txtLoader(file_path)
                file_documents = loader.load()
                documents.extend(file_documents)
                new_files.append(file)
                # æ›´æ–°æ–‡æ¡£ç´¢å¼•
                if incremental:
                    update_document_index(file_path)
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½æ–‡ä»¶ {file}ï¼Œè·³è¿‡æ­¤æ–‡ä»¶ã€‚é”™è¯¯ï¼š{e}")
            continue

    if incremental:
        print(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼š")
        print(f"  - æ–°å¤„ç†æ–‡ä»¶: {len(new_files)} ä¸ª")
        print(f"  - è·³è¿‡æœªä¿®æ”¹æ–‡ä»¶: {len(skipped_files)} ä¸ª")
        if new_files:
            print(f"  - æ–°æ–‡ä»¶åˆ—è¡¨: {', '.join(new_files)}")
        # if skipped_files:
        #     print(f"  - è·³è¿‡æ–‡ä»¶åˆ—è¡¨: {', '.join(skipped_files)}")
    else:
        print(f"å…¨é‡æ›´æ–°æ¨¡å¼ï¼šå¤„ç†äº† {len(documents)} ä¸ªæ–‡æ¡£")

    return documents

def split_documents(documents: list) -> list:
    """
    åªä¼ å…¥ documents åˆ—è¡¨ï¼Œå¯¹æ¯ä¸ªæ–‡æ¡£å…ˆæŒ‰ç« èŠ‚åˆ†ï¼Œå†æŒ‰å—åˆ†
    å†…éƒ¨ä½¿ç”¨é»˜è®¤è§„åˆ™ï¼ˆåˆ†ç« èŠ‚è§„åˆ™ + chunk_size + chunk_overlapï¼‰
    è¿”å›æ‰€æœ‰æ‹†åˆ†åçš„ Document å¯¹è±¡
    """

    # å›ºå®šå‚æ•°
    section_pattern = r"\n(?=\d{1,2}\s)"  # å¦‚ 1 èŒƒå›´ã€2 å¼•ç”¨æ–‡ä»¶ ç« èŠ‚åˆ’åˆ†çš„æ ¼å¼å¯ä»¥æ›´åŠ å®Œå–„ä¸€äº› ç›®å‰çš„æ­£åˆ™åŒ¹é…æ˜¯ æ¢è¡Œ+æ•°å­—+ç©ºæ ¼ (?=)æ˜¯æ­£å‘é¢„æŸ¥

    # æ‹†åˆ†å™¨é…ç½®
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", ". ", " ", ""], #æ„Ÿè§‰é—®å· æ„Ÿå¹å·ä»€ä¹ˆçš„è¿˜æ˜¯æ”¾ç½®ä¸€èµ·å¥½ 
        is_separator_regex=False
    )

    all_chunks = []
    seen_contents = set()  # ç”¨äºå»é‡çš„é›†åˆ

    for doc in documents:
        text = doc.page_content
        metadata = doc.metadata or {}

        # å…ˆæŒ‰ç« èŠ‚æ­£åˆ™åˆ†
        sections = re.split(section_pattern, text)

        for section in sections:
            cleaned = section.strip()
            if not cleaned:
                continue

            # çŸ­ç« èŠ‚ç›´æ¥ä¿ç•™
            if len(cleaned) <= CHUNK_SIZE:
                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒå†…å®¹
                if cleaned not in seen_contents:
                    all_chunks.append(Document(page_content=cleaned, metadata=metadata))
                    seen_contents.add(cleaned)
            else:
                # é•¿ç« èŠ‚å†æ‹†å—
                sub_chunks = splitter.split_text(cleaned)
                for chunk in sub_chunks:
                    chunk_cleaned = chunk.strip()
                    if chunk_cleaned and chunk_cleaned not in seen_contents:
                        all_chunks.append(Document(page_content=chunk_cleaned, metadata=metadata))
                        seen_contents.add(chunk_cleaned)

    print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼šåŸå§‹chunksæ•°é‡ {len(all_chunks) + len(seen_contents) - len(set(seen_contents))}ï¼Œå»é‡å {len(all_chunks)} ä¸ªchunks")
    return all_chunks

# 3. åˆå§‹åŒ–HuggingFaceåµŒå…¥æ¨¡å‹ é…ç½®gpuåŠ é€Ÿ  è¿”å›æ–‡æœ¬å‘é‡åŒ–å™¨ 
def initialize_embeddings():
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_PATH,
        model_kwargs={'device': EMBEDDING_DEVICE},
    )

# 4. åˆ›å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“ (Modified) æ£€æµ‹æ•°æ®åº“çŠ¶æ€  å¤„ç†æ¨¡å‹å˜æ›´å¯¼è‡´çš„ç»´åº¦é—®é¢˜ æ”¯æŒå¢é‡æ›´æ–°æ–‡æ¡£
def get_vector_db(chunks, embeddings, persist_directory):
    """Creates a new vector DB or loads an existing one."""
    if os.path.exists(persist_directory) and os.listdir(persist_directory): #æœ‰ç°æˆæ•°æ®åº“å°±åŠ åœ¨ï¼Œæ²¡æœ‰å°±çœ‹æœ‰æ²¡æœ‰chunkï¼Œæœ‰å°±åˆ›å»º 
        print(f"åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“ï¼š {persist_directory}...")
        try:
            # When loading, ChromaDB will check for dimension compatibility.
            # If EMBEDDING_MODEL_PATH changed leading to a dimension mismatch, this will fail.
            return Chroma(persist_directory=persist_directory, embedding_function=embeddings) #chromaä¼šä»ç›®å½•ä¸­æ‰¾æ•°æ®æ–‡ä»¶  å¹¶ä¸”å¯ç”¨embedding_functionå»é…ç½®
        except Exception as e: #ç¡®ä¿embeddingæ¨¡å‹å¯¹çš„
            print(f"æ— æ³•åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“ï¼š {e}.")
            # If loading fails, proceed as if it doesn't exist, but only create if chunks are given later.
            return None # Indicate loading failed or DB doesn't exist in a usable state
    else:
        # Directory doesn't exist or is empty
        if chunks:
            print(f"æ„å»ºæ–°çš„å‘é‡æ•°æ®åº“ï¼š {persist_directory}...")
            print(f"æ„å»ºChroma DB åŒ…å« {len(chunks)} ä¸ªchunks...")
            try:
                vector_db = Chroma.from_documents( #é€šè¿‡from_documentæŠŠæ¯ä¸ªchunkè½¬æ¢ä¸ºembeddingå¹¶å­˜åˆ°æ•°æ®åº“
                    documents=chunks,
                    embedding=embeddings,
                    persist_directory=persist_directory #è¿™æ˜¯ä¿å­˜ä½ç½®
                )
                return vector_db
            except Exception as e:
                print(f"åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                raise  # Re-raise the exception if creation fails
        else:
            # No chunks provided and DB doesn't exist/is empty - cannot create.
            print(f"å‘é‡æ•°æ®åº“ç›®å½• {persist_directory} ä¸å­˜åœ¨æˆ–ä¸ºç©º, ä¸”æ²¡æœ‰æä¾›chunksæ¥åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“ã€‚")
            return None # Indicate DB doesn't exist and cannot be created yet

def initialize_chat_history_vector_db():
    """åˆå§‹åŒ–å¯¹è¯å†å²å‘é‡æ•°æ®åº“"""
    global chat_history_vector_db, embeddings
    
    if embeddings is None:
        print("é”™è¯¯ï¼šEmbeddings æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºå¯¹è¯å†å²å‘é‡æ•°æ®åº“")
        return None
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(CHAT_HISTORY_PERSIST_DIR):
            os.makedirs(CHAT_HISTORY_PERSIST_DIR)
            print(f"åˆ›å»ºå¯¹è¯å†å²å‘é‡æ•°æ®åº“ç›®å½•: {CHAT_HISTORY_PERSIST_DIR}")
        
        # å°è¯•åŠ è½½ç°æœ‰çš„å¯¹è¯å†å²å‘é‡æ•°æ®åº“
        if os.path.exists(CHAT_HISTORY_PERSIST_DIR) and os.listdir(CHAT_HISTORY_PERSIST_DIR):
            print(f"åŠ è½½ç°æœ‰å¯¹è¯å†å²å‘é‡æ•°æ®åº“: {CHAT_HISTORY_PERSIST_DIR}")
            chat_history_vector_db = Chroma(persist_directory=CHAT_HISTORY_PERSIST_DIR, embedding_function=embeddings)
        else:
            print(f"åˆ›å»ºæ–°çš„å¯¹è¯å†å²å‘é‡æ•°æ®åº“: {CHAT_HISTORY_PERSIST_DIR}")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„å‘é‡æ•°æ®åº“
            chat_history_vector_db = Chroma(
                embedding_function=embeddings,
                persist_directory=CHAT_HISTORY_PERSIST_DIR
            )
        
        print("å¯¹è¯å†å²å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        return chat_history_vector_db
        
    except Exception as e:
        print(f"åˆå§‹åŒ–å¯¹è¯å†å²å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        return None

# 5. åˆå§‹åŒ–è¿æ¥åˆ°VLLMæœåŠ¡å™¨çš„ChatOpenAIå®¢æˆ·ç«¯ (Replaces initialize_llm) è¿æ¥VLLMæ¨ç†æœåŠ¡å™¨ é…ç½®æ¨¡å‹ è¿”å›å…¼å®¹æ¥å£
def initialize_openai_client():
    """Initializes ChatOpenAI client pointing to the VLLM server."""
    print(f"åˆå§‹åŒ–ChatOpenAI client æŒ‡å‘VLLMæœåŠ¡å™¨ {VLLM_BASE_URL}...")
    return ChatOpenAI(
        openai_api_base=VLLM_BASE_URL,
        openai_api_key=VLLM_API_KEY,
        model_name=VLLM_MODEL_NAME,
    )


#     return rag_chain
#ç›®å‰è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æ£€ç´¢æ–¹å¼ï¼Œè¿™ä¸ªå¸¦æœ‰è®°å¿†åŠŸèƒ½çš„å·²ç»è¢«ä¸¢å¼ƒï¼Œä½†æ˜¯ä¸ºäº†é˜²æ­¢æ–°çš„æ£€ç´¢ä¸å¯ç”¨ï¼Œå°±ä¿ç•™ä»– ç°åœ¨å·²ç»ä¸ä¼ å…¥memoryå‚æ•°
def create_rag_chain_with_memory(vector_db, llm, memory): #åˆ›å»ºå¸¦æœ‰è®°å¿†çš„é—®ç­”é“¾  è¿™éƒ¨åˆ†çš„æç¤ºè¯ç›¸å½“äºéƒ½æ˜¯å°è£…çš„ ç”¨åˆ«äººçš„åº“ å¯ä»¥å°è¯•è‡ªå·±æ‰‹å†™  
    retriever = vector_db.as_retriever(search_kwargs={"k": SEARCH_K}) #å°†å‘é‡æ•°æ®åº“å˜ä¸ºä¸€ä¸ªæ£€ç´¢å™¨ï¼Œèƒ½è¾“å…¥é—®é¢˜è¿”å›æœ€ç›¸å…³çš„kä¸ªæ–‡æœ¬
    # ç›´æ¥ç”¨ ConversationalRetrievalChainï¼Œè‡ªåŠ¨ç®¡ç†ä¸Šä¸‹æ–‡
    return ConversationalRetrievalChain.from_llm( #å°è£…çš„ç±»ï¼Œ å¯ä»¥æŠŠæ£€ç´¢å™¨è¿”å›çš„æ–‡æœ¬ï¼Œå½“å‰å¯¹è¯ å†å²å¯¹è¯æ‹¼æ¥æˆä¸€ä¸ªprompt   ï¼ˆæ£€ç´¢å‘é‡åº“å¾—åˆ°æœ€ç›¸å…³çš„kä¸ªï¼Œè·å–memory æ‹¼æ¥prompt LLMè°ƒç”¨è¿”å›ç»“æœï¼Œåˆ†åˆ«å¯¹åº”ä¸‹é¢çš„å‚æ•°ï¼‰
        llm=llm, #è¿™ä¸ªæŒ‡ç”¨å“ªä¸ªæ¨¡å‹å›ç­”
        retriever=retriever, #è¿™æ˜¯æŒ‡ç”¨å“ªä¸ªæ£€ç´¢å™¨ 
        memory=memory,
        return_source_documents=False  # å¦‚æœä¸éœ€è¦è¾“å‡ºæ£€ç´¢åˆ°çš„æºæ–‡æ¡£ï¼Œå¯è®¾ä¸º False 
    )

def create_dual_retrieval_rag_chain(document_vector_db, chat_history_vector_db, llm):
    """åˆ›å»ºæ”¯æŒåŒå‘é‡åº“æ£€ç´¢çš„RAGé“¾"""
    # åˆ›å»ºæ–‡æ¡£æ£€ç´¢å™¨
    document_retriever = document_vector_db.as_retriever(search_kwargs={"k": SEARCH_K})
    
    # åˆ›å»ºå¯¹è¯å†å²æ£€ç´¢å™¨
    chat_history_retriever = chat_history_vector_db.as_retriever(search_kwargs={"k": CHAT_HISTORY_SEARCH_K})
    
    # åˆ›å»ºè‡ªå®šä¹‰æç¤ºæ¨¡æ¿ï¼Œåˆ†åˆ«æ ‡æ³¨æ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²
    template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

ã€æ–‡æ¡£çŸ¥è¯†åº“å†…å®¹ã€‘ï¼š
{document_context}

ã€å†å²å¯¹è¯å†…å®¹ã€‘ï¼š
{chat_context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ï¼š"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["document_context", "chat_context", "question"]
    )
    
    # åˆ›å»ºLLMé“¾
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    
    # è¿”å›ä¸¤ä¸ªæ£€ç´¢å™¨å’ŒLLMé“¾ï¼Œè€Œä¸æ˜¯é›†æˆæ£€ç´¢å™¨
    return document_retriever, chat_history_retriever, llm_chain

# 7. Function to process query using the RAG chain (Modified for Streaming)
def process_query(query):
    """Processes a user query using the RAG chain and streams the answer."""
    global rag_chain, vector_db, chat_history_vector_db # Add vector_db to globals accessed here for debugging  è¿™é‡Œæ˜¯è¡¨æ˜ è¿™ä¸¤ä¸ªæ˜¯å…¨å±€å˜é‡ å¯ä»¥è®©å‡½æ•°å†…éƒ¨ä¿®æ”¹å‡½æ•°å¤–éƒ¨å®šä¹‰çš„å˜é‡ï¼ˆå¦‚æœæ²¡å®šä¹‰è¿™ä¸ªï¼Œå‡½æ•°å†…æ˜¯ä¸èƒ½ä¿®æ”¹å…¨å±€å˜é‡ï¼‰ ä¹Ÿå°±æ˜¯ä¹‹å‰å®šä¹‰è¿‡çš„ï¼Œå¦‚æœä¸å£°æ˜ ä¼šå‡ºé”™  
    if rag_chain is None:
        yield "é”™è¯¯ï¼šRAG é“¾æœªåˆå§‹åŒ–ã€‚"
        return
    print("DEBUG rag_chain:", rag_chain)
    print("DEBUG rag_chain.llm:", getattr(rag_chain, "llm", None))
    print("DEBUG rag_chain.retriever:", getattr(rag_chain, "retriever", None))

    # --- For Debugging Retrieval --- 
    # Uncomment the block below to see what documents are retrieved by the vector DB
    # if vector_db:
    #     try:
    #         retrieved_docs = vector_db.similarity_search(query, k=SEARCH_K)
    #         print(f"\n--- æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹: '{query}' ---")
    #         for i, doc in enumerate(retrieved_docs):
    #             # Attempt to get score if retriever supports it (Chroma's similarity_search_with_score)
    #             # For basic similarity_search, score might not be directly in metadata.
    #             # If using retriever.get_relevant_documents(), score might be present.
    #             score = doc.metadata.get('score', 'N/A') # å…ˆä»metadataä¸­å–score æ²¡æœ‰å°±è¿”å›NA
    #             if hasattr(doc, 'score'): #  æœ‰äº›documentæ˜¯å¸¦scoreå±æ€§  è¿™ä¸ªç›¸å½“äºæ˜¯ä¸€ç§å…¼å®¹æ€§ å¦‚æœå‰è€…æ²¡æ‰¾åˆ° å°±æ‰¾è¿™ä¸ª
    #                 score = doc.score
                
    #             print(f"æ–‡æ¡£ {i+1} (Score: {score}):")
    #             print(f"Content: {doc.page_content[:500]}...") # Print first 500 chars
    #             print(f"Metadata: {doc.metadata}")
    #         print("--- ç»“æŸæ–‡æ¡£æ£€ç´¢å†…å®¹ ---\n")
    #     except Exception as e:
    #         print(f"è°ƒè¯•æ–‡æ¡£æ£€ç´¢æ—¶å‡ºé”™: {e}")
    # else:
    #     print("æ–‡æ¡£å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–, è·³è¿‡è°ƒè¯•æ£€ç´¢ã€‚")
    
    # # è°ƒè¯•å¯¹è¯å†å²æ£€ç´¢
    # if chat_history_vector_db:
    #     try:
    #         retrieved_chat_docs = chat_history_vector_db.similarity_search(query, k=CHAT_HISTORY_SEARCH_K)
    #         print(f"\n--- æ£€ç´¢åˆ°çš„å¯¹è¯å†å²: '{query}' ---")
    #         for i, doc in enumerate(retrieved_chat_docs):
    #             score = doc.metadata.get('score', 'N/A')
    #             if hasattr(doc, 'score'):
    #                 score = doc.score
                
    #             print(f"å¯¹è¯å†å² {i+1} (Score: {score}):")
    #             print(f"Content: {doc.page_content[:500]}...")
    #             print(f"Metadata: {doc.metadata}")
    #         print("--- ç»“æŸå¯¹è¯å†å²æ£€ç´¢å†…å®¹ ---\n")
    #     except Exception as e:
    #         print(f"è°ƒè¯•å¯¹è¯å†å²æ£€ç´¢æ—¶å‡ºé”™: {e}")
    # else:
    #     print("å¯¹è¯å†å²å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–, è·³è¿‡è°ƒè¯•æ£€ç´¢ã€‚")
    # --- End Debugging Retrieval ---

    # ä½¿ç”¨æ–°çš„åŒå‘é‡åº“æ£€ç´¢ç³»ç»Ÿ è¿™ä¸ªæ˜¯ä¹‹å‰è¢«å¼ƒç”¨çš„ ç°åœ¨é‡æ–°å¯ç”¨


    try:
        # ä½¿ç”¨æ–°çš„åŒå‘é‡åº“æ£€ç´¢ç³»ç»Ÿ
        if hasattr(rag_chain, '__iter__') and len(rag_chain) == 3: #åˆ¤æ–­rag_chainæ˜¯å¦æœ‰å¯è¿­ä»£å¯¹è±¡hasattr(rag_chain, '__iter__') è¿™ä¸ªå†™æ³•å¯ä»¥è®°ä½
            # æ–°çš„åŒæ£€ç´¢ç³»ç»Ÿï¼šdocument_retriever, chat_history_retriever, llm_chain
            document_retriever, chat_history_retriever, llm_chain = rag_chain
            
            # åˆ†åˆ«ä»ä¸¤ä¸ªæ£€ç´¢å™¨è·å–å†…å®¹
            document_docs = document_retriever.get_relevant_documents(query)
            chat_docs = chat_history_retriever.get_relevant_documents(query)
            
            # æ‰“å°æ£€ç´¢ç»“æœç»Ÿè®¡
            print(f"æ–‡æ¡£æ£€ç´¢å™¨è¿”å›äº† {len(document_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            print(f"å¯¹è¯å†å²æ£€ç´¢å™¨è¿”å›äº† {len(chat_docs)} ä¸ªå¯¹è¯ç‰‡æ®µ")
            
            # æ£€æŸ¥æ–‡æ¡£æ£€ç´¢ç»“æœçš„å¤šæ ·æ€§
            unique_doc_contents = set()
            for i, doc in enumerate(document_docs):
                content_preview = doc.page_content[:100].strip()
                unique_doc_contents.add(content_preview)
                print(f"æ–‡æ¡£ç‰‡æ®µ {i+1}: {content_preview}...")
            
            print(f"æ–‡æ¡£æ£€ç´¢ç»“æœå¤šæ ·æ€§ï¼š{len(unique_doc_contents)}/{len(document_docs)} ä¸ªä¸åŒå†…å®¹")
            
            # åˆ†åˆ«åˆå¹¶æ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²å†…å®¹
            document_context = "\n\n".join([doc.page_content for doc in document_docs]) if document_docs else "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹"
            chat_context = "\n\n".join([doc.page_content for doc in chat_docs]) if chat_docs else "æœªæ‰¾åˆ°ç›¸å…³å†å²å¯¹è¯"
            
            print(f"æ–‡æ¡£ä¸Šä¸‹æ–‡: {document_context}")
            print(f"å¯¹è¯å†å²ä¸Šä¸‹æ–‡: {chat_context}")
            # æ‰“å°ä¸Šä¸‹æ–‡é•¿åº¦ä¿¡æ¯
            print(f"æ–‡æ¡£ä¸Šä¸‹æ–‡é•¿åº¦: {len(document_context)} å­—ç¬¦")
            print(f"å¯¹è¯å†å²ä¸Šä¸‹æ–‡é•¿åº¦: {len(chat_context)} å­—ç¬¦")
            
            # ç”Ÿæˆå›ç­”
            result = llm_chain.invoke({
                "document_context": document_context, 
                "chat_context": chat_context, 
                "question": query
            })
            
            if isinstance(result, dict) and "text" in result:
                yield result["text"]
            else:
                yield str(result)
        else:
            # å…¼å®¹æ—§çš„RAGé“¾
            result = rag_chain.invoke({ #rag_chainå°±æ˜¯ä¹‹å‰åˆ›å»ºçš„åŒ…æ‹¬æ£€ç´¢çš„æ•°æ® å†å²å›ç­” æ‹¼æ¥å¾—åˆ°çš„ç»“æœ streamå¯ä»¥ä¿è¯å›å¤æ˜¯ä¸€è¾¹ç”Ÿæˆä¸€è¾¹è¿”å›   invokeå’Œpredictéƒ½æ˜¯ä¸€æ¬¡æ€§è¿”å›æ•´æ®µå†…å®¹
                    "question": query,
                    #"enable_thinking": False
                                                })

           # result å¯èƒ½æ˜¯ dictï¼Œä¹Ÿå¯èƒ½ç›´æ¥æ˜¯å­—ç¬¦ä¸²ï¼Œè§† chain é…ç½®è€Œå®š
            if isinstance(result, dict) and "answer" in result:
                yield result["answer"]
            else:
                yield str(result)

    except Exception as e: # å…ˆç®€è¦æ‰“å°é”™è¯¯ä¿¡æ¯ ç„¶åé€šè¿‡traceback è¾“å‡ºå®Œæ•´çš„é”™è¯¯æ ˆè¿½è¸ª
        print(f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        yield f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {e}"


def safe_add_documents(vector_db, chunks, max_batch_size=5000): 
    """
    å®‰å…¨åˆ†æ‰¹æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
    
    å‚æ•°:
        vector_db: å·²åˆå§‹åŒ–çš„å‘é‡æ•°æ®åº“å¯¹è±¡
        chunks: å¾…æ·»åŠ çš„æ–‡æ¡£å—åˆ—è¡¨
        max_batch_size: å•æ¬¡æ‰¹é‡ä¸Šé™
    """
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹
    existing_contents = set()
    try:
        # è·å–ç°æœ‰æ–‡æ¡£çš„å†…å®¹ï¼ˆè¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®ChromaDBçš„APIè°ƒæ•´ï¼‰
        existing_docs = vector_db.similarity_search("", k=50000)  # è·å–æ‰€æœ‰æ–‡æ¡£
        for doc in existing_docs:
            existing_contents.add(doc.page_content.strip())
        print(f"å‘é‡æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_contents)} ä¸ªä¸åŒçš„æ–‡æ¡£å†…å®¹")
    except Exception as e:
        print(f"è·å–ç°æœ‰æ–‡æ¡£å†…å®¹æ—¶å‡ºé”™: {e}")
    
    # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„å†…å®¹
    new_chunks = []
    for chunk in chunks:
        content = chunk.page_content.strip()
        if content not in existing_contents:
            new_chunks.append(chunk)
    
    print(f"è¿‡æ»¤é‡å¤å†…å®¹ï¼šåŸå§‹ {len(chunks)} ä¸ªchunksï¼Œå»é‡å {len(new_chunks)} ä¸ªchunks")
    
    if not new_chunks:
        print("æ²¡æœ‰æ–°çš„æ–‡æ¡£éœ€è¦æ·»åŠ ")
        return
    
    for batch_start in range(0, len(new_chunks), max_batch_size):
        batch = new_chunks[batch_start : batch_start + max_batch_size]
        batch_num = (batch_start // max_batch_size) + 1
        
        try:
            print(f"ğŸ”„ æ­£åœ¨æ·»åŠ ç¬¬ {batch_num} æ‰¹ï¼ˆ{len(batch)} ä¸ªchunkï¼‰...")
            vector_db.add_documents(batch)
            print(f"âœ… ç¬¬ {batch_num} æ‰¹æ·»åŠ æˆåŠŸ")
        except Exception as e:
           
            print(f"âŒ ç¬¬ {batch_num} æ‰¹æ·»åŠ å¤±è´¥ï¼ˆæœ€ç»ˆå°è¯•ï¼‰ï¼š{str(e)}")
            raise  # æŠ›å‡ºå¼‚å¸¸ç»ˆæ­¢ç¨‹åº

# 8. Function to rebuild the index and RAG chain (Modified to add documents)
def rebuild_index_and_chain(incremental=True): #å…¨æµç¨‹ç´¢å¼•é‡å»º  æ–‡æ¡£åŠ è½½-åˆ†å‰²-åµŒå…¥-å­˜å‚¨   
    """
    åŠ è½½æ–‡æ¡£ï¼Œåˆ›å»º/æ›´æ–°å‘é‡æ•°æ®åº“ï¼Œå¹¶é‡å»ºRAGé“¾
    
    å‚æ•°:
        incremental: æ˜¯å¦å¯ç”¨å¢é‡æ›´æ–°æ¨¡å¼
    """
    global vector_db, rag_chain, embeddings, llm, chat_history_vector_db

    if embeddings is None or llm is None:
        return "é”™è¯¯ï¼šEmbeddings æˆ– LLM æœªåˆå§‹åŒ–ã€‚"

    # åŠ è½½æ–‡æ¡£ç´¢å¼•
    load_document_index()

    # åˆå§‹åŒ–å¯¹è¯å†å²å‘é‡æ•°æ®åº“
    if chat_history_vector_db is None:
        chat_history_vector_db = initialize_chat_history_vector_db()

    # Ensure documents directory exists
    if not os.path.exists(DOCUMENTS_DIR):
        os.makedirs(DOCUMENTS_DIR)
        print(f"åˆ›å»ºæ–‡æ¡£ç›®å½•: {DOCUMENTS_DIR}")

    # Step 1: Load documents
    print("åŠ è½½æ–‡æ¡£...")
    documents = load_documents(DOCUMENTS_DIR, incremental=incremental)
    if not documents:
        print(f"åœ¨ {DOCUMENTS_DIR} ä¸­æœªæ‰¾åˆ°æ–‡æ¡£ã€‚")
        # Try to load existing DB even if no new documents are found
        print("å°è¯•åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
        # Pass None for chunks as we are just trying to load
        vector_db = get_vector_db(None, embeddings, PERSIST_DIR)
        if vector_db:
            print("æ²¡æœ‰æ–°æ–‡æ¡£åŠ è½½ï¼Œå°†ä½¿ç”¨ç°æœ‰çš„å‘é‡æ•°æ®åº“ã€‚é‡æ–°åˆ›å»º RAG é“¾...")
            # ä½¿ç”¨æ–°çš„åŒå‘é‡åº“RAGé“¾
            if chat_history_vector_db:
                rag_chain = create_dual_retrieval_rag_chain(vector_db, chat_history_vector_db, llm)
            else:
                # å¦‚æœå¯¹è¯å†å²å‘é‡åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§çš„RAGé“¾
                rag_chain = create_rag_chain_with_memory(vector_db, llm, None)

            return "æ²¡æœ‰æ‰¾åˆ°æ–°æ–‡æ¡£ï¼Œå·²ä½¿ç”¨ç°æœ‰æ•°æ®é‡æ–°åŠ è½½ RAG é“¾ã€‚"
        else:
            # No documents AND no existing DB
            return "é”™è¯¯ï¼šæ²¡æœ‰æ–‡æ¡£å¯åŠ è½½ï¼Œä¸”æ²¡æœ‰ç°æœ‰çš„å‘é‡æ•°æ®åº“ã€‚"

    # Step 2: Split text
    print("åˆ†å‰²æ–‡æœ¬...")
    chunks = split_documents(documents)
    # è¿‡æ»¤å’Œé¢„å¤„ç†ï¼šåªä¿ç•™éç©ºå­—ç¬¦ä¸²å†…å®¹çš„chunk é˜²æ­¢è¾“å…¥çš„æ ¼å¼æœ‰é—®é¢˜ å¯¼è‡´åç»­çš„å‘é‡åº“æ·»åŠ å¤±è´¥ TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]
    filtered_chunks = []
    for c in chunks:
        if hasattr(c, 'page_content') and isinstance(c.page_content, str):
            content = c.page_content.strip()
            if content:
                c.page_content = content  # å»é™¤é¦–å°¾ç©ºç™½
                filtered_chunks.append(c)
    print(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_chunks)} ä¸ªæœ‰æ•ˆæ–‡æœ¬å—ï¼ˆåŸå§‹ {len(chunks)} ä¸ªï¼‰")
    chunks = filtered_chunks
    if not chunks:
        print("åˆ†å‰²åæœªç”Ÿæˆæ–‡æœ¬å—ã€‚")
        # Try loading existing DB if splitting yielded nothing
        print("å°è¯•åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
        vector_db = get_vector_db(None, embeddings, PERSIST_DIR)
        if vector_db:
             print("è­¦å‘Šï¼šæ–°åŠ è½½çš„æ–‡æ¡£åˆ†å‰²åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ã€‚ä½¿ç”¨ç°æœ‰æ•°æ®åº“ã€‚")
             # ä½¿ç”¨æ–°çš„åŒå‘é‡åº“RAGé“¾
             if chat_history_vector_db:
                 rag_chain = create_dual_retrieval_rag_chain(vector_db, chat_history_vector_db, llm)
             else:
                 # å¦‚æœå¯¹è¯å†å²å‘é‡åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§çš„RAGé“¾
                 rag_chain = create_rag_chain_with_memory(vector_db, llm, None)
             return "è­¦å‘Šï¼šæ–‡æ¡£åˆ†å‰²åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ã€‚RAG é“¾å·²ä½¿ç”¨ç°æœ‰æ•°æ®é‡æ–°åŠ è½½ã€‚"
        else:
            # No chunks AND no existing DB
            return "é”™è¯¯ï¼šæ–‡æ¡£åˆ†å‰²åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ï¼Œä¸”æ— ç°æœ‰æ•°æ®åº“ã€‚"

    # Step 3: Load or Create/Update vector database
    print("åŠ è½½æˆ–æ›´æ–°å‘é‡æ•°æ®åº“...")
    # Try loading first, even if we have chunks (in case we want to add to it)
    vector_db_loaded = get_vector_db(None, embeddings, PERSIST_DIR) #Noneç”¨äºåŠ è½½ä¸€ä¸ªå·²ç»å­˜åœ¨çš„æ•°æ®åº“ ä¸æƒ³æ–°å»º 

    if vector_db_loaded:
        print(f"å‘ç°æœ‰å‘é‡æ•°æ®åº“æ·»åŠ  {len(chunks)} ä¸ªå—...")
        vector_db = vector_db_loaded # Use the loaded DB
        try:
            # Consider adding only new chunks if implementing duplicate detection later
            print("å¼€å§‹æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“æ¯æ¬¡é™åˆ¶æœ€å¤šäº”åƒä¸ª...")
            safe_add_documents(vector_db, chunks, max_batch_size=5000)
            print("å—æ·»åŠ æˆåŠŸã€‚")
            # Persisting might be needed depending on Chroma version/setup, often automatic.
            # vector_db.persist() # Uncomment if persistence issues occur
        except Exception as e:
             print(f"æ·»åŠ æ–‡æ¡£åˆ° Chroma æ—¶å‡ºé”™: {e}")
             # If adding fails, proceed with the DB as it was before adding
             # ä½¿ç”¨æ–°çš„åŒå‘é‡åº“RAGé“¾
             if chat_history_vector_db:
                 rag_chain = create_dual_retrieval_rag_chain(vector_db, chat_history_vector_db, llm)
             else:
                 # å¦‚æœå¯¹è¯å†å²å‘é‡åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§çš„RAGé“¾
                 rag_chain = create_rag_chain_with_memory(vector_db, llm, None)
             return f"é”™è¯¯ï¼šå‘å‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£æ—¶å‡ºé”™: {e}ã€‚RAGé“¾å¯èƒ½ä½¿ç”¨æ—§æ•°æ®ã€‚"
    else:
        # Database didn't exist or couldn't be loaded, create a new one with the current chunks
        print(f"åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“å¹¶æ·»åŠ  {len(chunks)} ä¸ªå—...")
        try:
            # Call get_vector_db again, this time *with* chunks to trigger creation
            vector_db = get_vector_db(chunks, embeddings, PERSIST_DIR) #ç”¨äºæ–°å»ºæ•°æ®åº“
            if vector_db is None: # Check if creation failed within get_vector_db
                 raise RuntimeError("get_vector_db failed to create a new database.")
            print("æ–°çš„å‘é‡æ•°æ®åº“å·²åˆ›å»ºå¹¶æŒä¹…åŒ–ã€‚")
        except Exception as e:
            print(f"åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            return f"é”™è¯¯ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}"

    if vector_db is None:
         # This should ideally not be reached if error handling above is correct
         return "é”™è¯¯ï¼šæœªèƒ½åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“ã€‚"

    # Step 4: Create RAG chain
    print("åˆ›å»º RAG é“¾...")
    # ä½¿ç”¨æ–°çš„åŒå‘é‡åº“RAGé“¾
    if chat_history_vector_db:
        rag_chain = create_dual_retrieval_rag_chain(vector_db, chat_history_vector_db, llm)
    else:
        # å¦‚æœå¯¹è¯å†å²å‘é‡åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§çš„RAGé“¾
        rag_chain = create_rag_chain_with_memory(vector_db, llm, None)
    
    # ä¿å­˜æ–‡æ¡£ç´¢å¼•
    if incremental:
        save_document_index()
    
    print("ç´¢å¼•å’Œ RAG é“¾å·²æˆåŠŸæ›´æ–°ã€‚")
    return "æ–‡æ¡£å¤„ç†å®Œæˆï¼Œç´¢å¼•å’Œ RAG é“¾å·²æ›´æ–°ã€‚"

def force_rebuild_index():
    """å¼ºåˆ¶å…¨é‡é‡å»ºç´¢å¼•ï¼ˆå¿½ç•¥æ–‡æ¡£ç´¢å¼•ï¼‰"""
    return rebuild_index_and_chain(incremental=False)

def get_document_index_status():
    """è·å–æ–‡æ¡£ç´¢å¼•çŠ¶æ€ä¿¡æ¯"""
    load_document_index()
    
    if not document_index:
        return "ğŸ“‹ æ–‡æ¡£ç´¢å¼•çŠ¶æ€ï¼š\n- æš‚æ— å·²å¤„ç†çš„æ–‡æ¡£è®°å½•"
    
    total_files = len(document_index)
    total_size = sum(info.get("size", 0) for info in document_index.values())
    total_size_mb = total_size / (1024 * 1024)
    
    status = f"ğŸ“‹ æ–‡æ¡£ç´¢å¼•çŠ¶æ€ï¼š\n"
    status += f"- å·²å¤„ç†æ–‡æ¡£æ•°é‡ï¼š{total_files} ä¸ª\n"
    status += f"- æ€»æ–‡ä»¶å¤§å°ï¼š{total_size_mb:.2f} MB\n"
    status += f"- ç´¢å¼•æ–‡ä»¶ï¼š{DOCUMENT_INDEX_FILE}\n\n"
    
    # åˆ—å‡ºæ‰€æœ‰å·²å¤„ç†çš„æ–‡ä»¶
    status += "ğŸ“„ å·²å¤„ç†æ–‡æ¡£åˆ—è¡¨ï¼š\n"
    for file_path, info in document_index.items():
        file_name = os.path.basename(file_path)
        file_size_mb = info.get("size", 0) / (1024 * 1024)
        mtime = datetime.fromtimestamp(info.get("mtime", 0)).strftime("%Y-%m-%d %H:%M:%S")
        status += f"- {file_name} ({file_size_mb:.2f} MB, ä¿®æ”¹æ—¶é—´: {mtime})\n"
    
    return status

def get_chat_history_status():
    """è·å–å¯¹è¯å†å²å‘é‡åº“çŠ¶æ€ä¿¡æ¯"""
    global chat_history_vector_db
    
    if chat_history_vector_db is None:
        return "ğŸ“‹ å¯¹è¯å†å²å‘é‡åº“çŠ¶æ€ï¼š\n- å¯¹è¯å†å²å‘é‡åº“æœªåˆå§‹åŒ–"
    
    try:
        # è·å–å¯¹è¯å†å²æ•°é‡ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®ChromaDBçš„APIæ¥è·å–ï¼‰
        # ç”±äºChromaDBæ²¡æœ‰ç›´æ¥çš„æ–¹æ³•è·å–æ–‡æ¡£æ•°é‡ï¼Œæˆ‘ä»¬é€šè¿‡æ£€ç´¢æ¥ä¼°ç®—
        test_docs = chat_history_vector_db.similarity_search("", k=1000)
        total_dialogues = len(test_docs)
        
        status = f"ğŸ“‹ å¯¹è¯å†å²å‘é‡åº“çŠ¶æ€ï¼š\n"
        status += f"- å¯¹è¯å†å²æ•°é‡ï¼š{total_dialogues} æ¡\n"
        status += f"- å‘é‡åº“è·¯å¾„ï¼š{CHAT_HISTORY_PERSIST_DIR}\n\n"
        
        # åˆ—å‡ºæœ€è¿‘çš„å¯¹è¯å†å²
        if total_dialogues > 0:
            status += "ğŸ’¬ æœ€è¿‘çš„å¯¹è¯å†å²ï¼š\n"
            recent_docs = chat_history_vector_db.similarity_search("", k=min(5, total_dialogues))
            for i, doc in enumerate(recent_docs):
                timestamp = doc.metadata.get("timestamp", "æœªçŸ¥æ—¶é—´")
                content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                status += f"- å¯¹è¯ {i+1} ({timestamp}): {content_preview}\n"
        
        return status
        
    except Exception as e:
        return f"ğŸ“‹ å¯¹è¯å†å²å‘é‡åº“çŠ¶æ€ï¼š\n- è·å–çŠ¶æ€æ—¶å‡ºé”™: {e}"

def clear_chat_history_vector_db():
    """æ¸…ç©ºå¯¹è¯å†å²å‘é‡åº“"""
    global chat_history_vector_db
    
    try:
        if chat_history_vector_db is not None:
            # åˆ é™¤å¯¹è¯å†å²å‘é‡åº“ç›®å½•
            import shutil
            if os.path.exists(CHAT_HISTORY_PERSIST_DIR):
                shutil.rmtree(CHAT_HISTORY_PERSIST_DIR)
                print(f"å·²åˆ é™¤å¯¹è¯å†å²å‘é‡åº“ç›®å½•: {CHAT_HISTORY_PERSIST_DIR}")
            
            # é‡æ–°åˆå§‹åŒ–å¯¹è¯å†å²å‘é‡åº“
            chat_history_vector_db = initialize_chat_history_vector_db()
            
            # é‡æ–°åˆ›å»ºRAGé“¾
            if vector_db is not None and chat_history_vector_db is not None:
                rag_chain = create_dual_retrieval_rag_chain(vector_db, chat_history_vector_db, llm)
            
            return "å¯¹è¯å†å²å‘é‡åº“å·²æ¸…ç©ºå¹¶é‡æ–°åˆå§‹åŒ–"
        else:
            return "å¯¹è¯å†å²å‘é‡åº“æœªåˆå§‹åŒ–"
            
    except Exception as e:
        return f"æ¸…ç©ºå¯¹è¯å†å²å‘é‡åº“æ—¶å‡ºé”™: {e}"

# Helper function to list documents in the directory ç”Ÿæˆå·²åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨ markdownæ ¼å¼åŒ–è¾“å‡º  å®æ—¶æ›´æ–°æ–‡æ¡£çŠ¶æ€
def get_loaded_documents_list():
    """Returns an HTML formatted list of files in DOCUMENTS_DIR."""
    if not os.path.exists(DOCUMENTS_DIR) or not os.listdir(DOCUMENTS_DIR): #å­˜åœ¨åˆ™å‡½æ•°è¿”å›true  å¦‚æœæ–‡æ¡£å­˜åœ¨ï¼Œå¹¶ä¸”é‡Œé¢æœ‰ä¸œè¥¿å°±ä¸è¿”å›å†…å®¹
        return "å½“å‰æ²¡æœ‰å·²åŠ è½½çš„æ–‡æ¡£ã€‚"
    try:
        files = [f for f in os.listdir(DOCUMENTS_DIR) if os.path.isfile(os.path.join(DOCUMENTS_DIR, f)) and (f.endswith('.pdf') or f.endswith('.docx') or f.endswith('.doc'))]# ç¬¦åˆæ¡ä»¶çš„å…¨éƒ¨éå†
        if not files: 
            return "å½“å‰æ²¡æœ‰å·²åŠ è½½çš„æ–‡æ¡£ã€‚"
        
        # ç”ŸæˆHTMLæ ¼å¼çš„æ–‡æ¡£åˆ—è¡¨
        html_list = "<ul>"
        for file in files:
            html_list += f"<li>{file}</li>"
        html_list += "</ul>"
        return html_list
    except Exception as e:
        print(f"Error listing documents: {e}")
        return "æ— æ³•åˆ—å‡ºæ–‡æ¡£ã€‚"


# 9. Function to handle file uploads (Modified to return doc list) æ¥å—ä¸Šä¼ çš„æ–‡æ¡£ ä¿å­˜åˆ°æ–‡æ¡£ç›®å½• è§¦å‘ç´¢å¼•é‡å»ºï¼ˆç±»ä¼¼å›¾ä¹¦é¦†è¿›æ¥æ–°çš„ä¹¦ é‡æ–°åˆ›å»ºç´¢å¼•ï¼‰ è¿”å›ä¸Šä¼ çŠ¶æ€
def handle_file_upload(file_obj, incremental=True):
    """
    ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ï¼Œè§¦å‘ç´¢å¼•é‡å»ºï¼Œå¹¶è¿”å›çŠ¶æ€å’Œæ–‡æ¡£åˆ—è¡¨
    
    å‚æ•°:
        file_obj: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
        incremental: æ˜¯å¦å¯ç”¨å¢é‡æ›´æ–°æ¨¡å¼
    """
    if file_obj is None:
        return "æœªé€‰æ‹©æ–‡ä»¶ã€‚", get_loaded_documents_list() # Return current list even if no file selected

    try:
        # Gradio provides a temporary file path
        temp_file_path = file_obj.name
        file_name = os.path.basename(temp_file_path)
        destination_path = os.path.join(DOCUMENTS_DIR, file_name)

        print(f"å°†ä¸Šä¼ çš„æ–‡ä»¶ä» {temp_file_path} å¤åˆ¶åˆ° {destination_path}")
        # Ensure documents directory exists
        if not os.path.exists(DOCUMENTS_DIR):
            os.makedirs(DOCUMENTS_DIR)
        shutil.copy(temp_file_path, destination_path) # Copy the file

        print(f"æ–‡ä»¶ {file_name} ä¸Šä¼ æˆåŠŸã€‚å¼€å§‹é‡å»ºç´¢å¼•...")
        status = rebuild_index_and_chain(incremental=incremental)
        final_status = f"æ–‡ä»¶ '{file_name}' ä¸Šä¼ æˆåŠŸã€‚\n{status}"
        # Get updated document list
        doc_list_md = get_loaded_documents_list()
        return final_status, doc_list_md

    except Exception as e:
        print(f"æ–‡ä»¶ä¸Šä¼ æˆ–å¤„ç†å¤±è´¥: {e}")
        # Return error and current doc list
        return f"æ–‡ä»¶ä¸Šä¼ æˆ–å¤„ç†å¤±è´¥: {e}", get_loaded_documents_list()

def handle_memory_and_query_prep(query_text, current_user_facts): #å‰è€…æ˜¯æé—® åè€…æ˜¯ç›®å‰å­˜å‚¨çš„å†…å®¹  è¿™ä¸ªå‡½æ•°åªä¼šå­˜å‚¨è¦æ±‚è®°ä½çš„å†…å®¹ å¦‚æœæ˜¯æé—®ï¼Œåªä¼šæŠŠä¹‹å‰è¦æ±‚è®°ä½çš„å†…å®¹å’Œæé—®æ‹¼æ¥èµ·æ¥è¿”å›å›å»
    """Handles the memory feature and prepares the full query."""
    # Create a mutable copy of user_facts to modify within this function
    updated_user_facts = list(current_user_facts)

    if "è®°ä½" in query_text:
        # æå–è®°ä½çš„å†…å®¹ï¼ˆå»æ‰"è¯·è®°ä½"ã€"è®°ä½"ç­‰å‰ç¼€ï¼‰
        fact = query_text.replace("è¯·è®°ä½", "").replace("è®°ä½", "").strip("ï¼š:ï¼Œ,ã€‚. ")
        if fact:
            updated_user_facts.append(fact)
            # No chat history update here, that's for the UI layer.
            return "", updated_user_facts # Indicate it's a memory command, no query for RAG

    # æ‹¼æ¥æ‰€æœ‰è®°å¿†å†…å®¹åˆ°ç”¨æˆ·è¾“å…¥å‰é¢
    if updated_user_facts: #è¿™éƒ¨åˆ†çš„memory_prefixéƒ½æ˜¯ç”¨æˆ·è¦æ±‚è®°ä½çš„å†…å®¹ï¼Œå¯ä»¥ç†è§£ä¸ºé—®é¢˜é‡å†™äº†ä¸€éï¼Œä½†è‡³å°‘å¢åŠ äº†ä¸€éƒ¨åˆ†è¦æ±‚è®°ä½çš„å†…å®¹
        memory_prefix = "ï¼Œ".join(updated_user_facts)
        full_query = f"è¯·è®°ä½ï¼š{memory_prefix}ã€‚ç”¨æˆ·æé—®ï¼š{query_text}"
    else:
        full_query = query_text

    return full_query, updated_user_facts


# Updated function to handle query submission for gr.Chatbot ç®¡ç†èŠå¤©å†å²  æ˜¾ç¤ºæ€è€ƒä¸­çŠ¶æ€ æ›´æ–°é—®ç­” æ¸…ç©ºè¾“å…¥æ¡†
def handle_submit_with_thinking(query_text, chat_history):
    global user_facts, chat_history_vector_db
    if chat_history:
        previous_question = chat_history[-1][0]
    else:
        previous_question = ""
    rewritten_query = rewrite_question_if_needed(query_text, previous_question)
    #æµ‹è¯•ä½¿ç”¨ åæœŸå¯ä»¥åˆ æ‰
    if rewritten_query != query_text:
        print(f"é—®é¢˜å·²æ”¹å†™: {rewritten_query}")
    else:
        print(f"é—®é¢˜æ²¡æœ‰æ”¹å†™:{rewritten_query}")
    query_to_use = rewritten_query 

    # Call the new function to handle memory and prepare the query
    full_query, updated_user_facts = handle_memory_and_query_prep(query_to_use, user_facts)

    user_facts[:] = updated_user_facts
    if full_query == "":
        if "è®°ä½" in query_text:
            # æå–è®°ä½çš„å†…å®¹ï¼ˆå»æ‰"è¯·è®°ä½"ã€"è®°ä½"ç­‰å‰ç¼€ï¼‰
            fact = query_text.replace("è¯·è®°ä½", "").replace("è®°ä½", "").strip("ï¼š:ï¼Œ,ã€‚. ")
            if fact:
                user_facts.append(fact)
                chat_history.append((query_text, f"å¥½çš„ï¼Œæˆ‘å·²è®°ä½ï¼š{fact}"))
                yield "", chat_history
                return

    if not query_text or query_text.strip() == "":
        yield "", chat_history
        return
  
    # æ‹¼æ¥æ‰€æœ‰è®°å¿†å†…å®¹åˆ°ç”¨æˆ·è¾“å…¥å‰é¢  è¿™é‡Œå¯ä»¥ä¿®æ”¹ä¸€ä¸‹ æ¯”å¦‚ç»™ä¸€ä¸ªé¡ºåºï¼Œæ²¡ç»è¿‡å‡ æ¬¡å¯¹è¯ï¼Œå°±åˆ æ‰ä¹‹å‰çš„å†…å®¹ é˜²æ­¢å†…å®¹å¤ªå¤š å¯¼è‡´è¶…è¿‡æœ€å¤§é•¿åº¦
    if user_facts:  
        memory_prefix = "ï¼Œ".join(user_facts)
        full_query = f"è¯·è®°ä½ï¼š{memory_prefix}ã€‚æ”¹å†™åçš„é—®é¢˜ï¼š{query_to_use}" #è¿™ä¸ªæ˜¯å°†ç”¨æˆ·çš„æé—®å’Œå›ç­”æ”¾åœ¨ä¸€èµ·ç”¨äºè®°å¿†
    else:
        full_query = query_to_use

    chat_history.append((query_text, "æ€è€ƒä¸­..."))
    yield "", chat_history

    final_response_from_rag = "æ€è€ƒä¸­..."

    for stream_chunk in process_query(full_query):
        final_response_from_rag = stream_chunk
        chat_history[-1] = (query_text, final_response_from_rag) #å› ä¸ºchat_historyæ˜¯ä¸€ä¸ªäºŒå…ƒç»„ è¿™ä¸ªæ„æ€æ˜¯ç”¨åé¢çš„å†…å®¹æ›¿ä»£å†å²å¯¹è¯çš„æœ€è¿‘ä¸€æ¬¡å†…å®¹
        yield "", chat_history

    if chat_history and chat_history[-1][1] == "æ€è€ƒä¸­...": #å¦‚æœå†…å®¹å…¨æ˜¯æ€è€ƒä¸­å°±ç›´æ¥passæ‰
        pass
 
    # æ–°å¢ï¼šå°†æœ¬è½®å¯¹è¯å­˜å…¥å¯¹è¯å†å²å‘é‡åº“
    if chat_history_vector_db is not None and query_text.strip() and final_response_from_rag.strip(): #stripå»æ‰å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦
        # ç»„åˆæˆä¸€ä¸ªç‰‡æ®µ
        dialogue_text = f"ç”¨æˆ·: {query_text}\nAI: {final_response_from_rag}"
        # åˆ›å»º Document å¯¹è±¡
        doc = Document(page_content=dialogue_text, metadata={"type": "chat_history", "timestamp": datetime.now().isoformat()}) #æ‰“ä¸ªæ ‡ç­¾ è¡¨ç¤ºè¿™ä¸ªæ˜¯å¯¹è¯å†å²
        try: 
            chat_history_vector_db.add_documents([doc]) #å°†å¯¹è¯å†å²å­˜å‚¨åˆ°ä¸“é—¨çš„å‘é‡åº“ä¸­
            print("å¯¹è¯å·²å­˜å…¥å¯¹è¯å†å²å‘é‡åº“") 
        except Exception as e:
            print(f"å­˜å‚¨å¯¹è¯åˆ°å¯¹è¯å†å²å‘é‡åº“å¤±è´¥: {e}")

# 10. åˆå§‹åŒ–ç³»ç»Ÿå‡½æ•°
def initialize_system():
    """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    global embeddings, llm, rag_chain, chat_history_vector_db

    print(f"IMPORTANT: Current embedding model is {EMBEDDING_MODEL_PATH}.")
    print(f"If you recently changed the embedding model and encounter dimension mismatch errors,")
    print(f"you MUST manually delete the ChromaDB directory: '{PERSIST_DIR}' and restart.")

    # Ensure documents directory exists at start
    if not os.path.exists(DOCUMENTS_DIR):
        os.makedirs(DOCUMENTS_DIR)
        print(f"åˆ›å»ºæ–‡æ¡£ç›®å½•: {DOCUMENTS_DIR}")
        print("è¯·å°†æ‚¨çš„ PDF å’Œ DOCX æ–‡ä»¶æ·»åŠ åˆ°æ­¤ç›®å½•æˆ–ä½¿ç”¨ä¸Šä¼ åŠŸèƒ½ã€‚")

    # Initialize embeddings and LLM once
    print("åˆå§‹åŒ– Embedding æ¨¡å‹...")
    embeddings = initialize_embeddings()

    print("åˆå§‹åŒ– LLM å®¢æˆ·ç«¯...")
    llm = initialize_openai_client()

    # åˆå§‹åŒ–å¯¹è¯å†å²å‘é‡æ•°æ®åº“
    print("åˆå§‹åŒ–å¯¹è¯å†å²å‘é‡æ•°æ®åº“...")
    chat_history_vector_db = initialize_chat_history_vector_db()

    # Initial index and chain build
    print("æ‰§è¡Œåˆå§‹ç´¢å¼•æ„å»º...")
    initial_status = rebuild_index_and_chain()
    print(initial_status)
    
    if vector_db is not None:
        # ä½¿ç”¨æ–°çš„åŒå‘é‡åº“RAGé“¾
        if chat_history_vector_db:
            rag_chain = create_dual_retrieval_rag_chain(vector_db, chat_history_vector_db, llm)
        else:
            # å¦‚æœå¯¹è¯å†å²å‘é‡åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§çš„RAGé“¾
            rag_chain = create_rag_chain_with_memory(vector_db, llm, None)
    else:
        print("è­¦å‘Šï¼šå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼ŒRAG é“¾å¯èƒ½ä¸å¯ç”¨ã€‚")

    return initial_status

# 11. ä¸»å‡½æ•°ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºç›´æ¥è¿è¡ŒRAGç³»ç»Ÿ"""
    initialize_system()
    print("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆã€‚")

if __name__ == "__main__":
    main()