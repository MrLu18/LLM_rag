import os
import re
import shutil
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import Document
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from transformers import AutoTokenizer
from rag_new.config import (
    DOCUMENTS_DIR,
    PERSIST_DIR,
    EMBEDDING_MODEL_PATH,
    EMBEDDING_DEVICE,
    VLLM_BASE_URL,
    VLLM_API_KEY,
    VLLM_MODEL_NAME,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    SEARCH_K
)

# Global variables
rag_chain = None
vector_db = None
embeddings = None
llm = None

# Tokenizeré…ç½®ï¼ˆå¦‚æœ‰éœ€è¦å¯æ›´æ¢ä¸ºä½ çš„æ¨¡å‹åï¼‰
TOKENIZER_MODEL = "/mnt/jrwbxx/LLM/model/qwen3-1.7b" #è¿™ä¸ªè®°å¾—éšç€æ¨¡å‹è€Œå˜
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)

MAX_TURNS = 3
MAX_TOKENS = 512

memory = ConversationBufferWindowMemory(
    memory_key="chat_history",
    return_messages=True,
    k=MAX_TURNS
)
user_facts = []  #è¿™ä¸ªå˜é‡åªä¼šå­˜å‚¨æé—®çš„é—®é¢˜ ä¸åŒ…å«å›ç­”çš„é—®é¢˜



def trim_memory_tokens(memory_obj): #æ­¤å‡½æ•°è¦æ±‚å¯¹è¯å†å²ä¸è¶…è¿‡ä¸‰è½®ï¼Œæ€»tokenä¸è¶…è¿‡1024 ä¸€ä¸ªè¶…è¿‡å°±è£å‰ªæœ€è¿œçš„å¯¹è¯ï¼Œä½†æ˜¯æœ€ç»ˆä¼šè‡³å°‘ä¿å­˜ä¸€è½®å¯¹è¯
    """è£å‰ªmemory_objä¸­çš„å†å²ï¼Œä½¿å…¶tokenæ•°ä¸è¶…è¿‡MAX_TOKENS"""
    history = memory_obj.chat_memory.messages
    # åªä¿ç•™æœ€è¿‘MAX_TURNSè½®
    if len(history) > MAX_TURNS * 2:
        history = history[-MAX_TURNS*2:]
    # æ‹¼æ¥æˆå­—ç¬¦ä¸²
    history_text = ""
    for msg in history:  
        if hasattr(msg, "content"): #å¦‚æœå†å²å¯¹è¯æœ‰contentå±æ€§ åˆ™ç›´æ¥ç”¨ æ²¡æœ‰å°±è½¬æ¢ä¸ºå­—ç¬¦ä¸² hasattræ˜¯ä¸€ä¸ªå†…ç½®å‡½æ•° æ£€æŸ¥å¯¹è±¡æœ‰æ²¡æœ‰å¯¹åº”çš„å±æ€§
            history_text += msg.content + "\n"
        else:
            history_text += str(msg) + "\n"
    input_ids = tokenizer(history_text, return_tensors="pt").input_ids #ç”¨tokenizerå°†å†å²æ–‡æœ¬è½¬æ¢ä¸ºtokenID 
    while input_ids.shape[1] > MAX_TOKENS and len(history) > 2: #ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå¯¹è¯å†å² 
        history = history[2:]
        history_text = ""
        for msg in history:
            if hasattr(msg, "content"):
                history_text += msg.content + "\n"
            else:
                history_text += str(msg) + "\n"
        input_ids = tokenizer(history_text, return_tensors="pt").input_ids
    memory_obj.chat_memory.messages = history


def handle_memory_and_query_prep(query_text, current_user_facts): #å‰è€…æ˜¯æé—® åè€…æ˜¯ç›®å‰å­˜å‚¨çš„å†…å®¹  è¿™ä¸ªå‡½æ•°åªä¼šå­˜å‚¨è¦æ±‚è®°ä½çš„å†…å®¹ å¦‚æœæ˜¯æé—®ï¼Œåªä¼šæŠŠä¹‹å‰è¦æ±‚è®°ä½çš„å†…å®¹å’Œæé—®æ‹¼æ¥èµ·æ¥è¿”å›å›å»
    """Handles the memory feature and prepares the full query."""
    # Create a mutable copy of user_facts to modify within this function
    updated_user_facts = list(current_user_facts)

    if "è®°ä½" in query_text:
        # æå–è®°ä½çš„å†…å®¹ï¼ˆå»æ‰"è¯·è®°ä½"ã€"è®°ä½"ç­‰å‰ç¼€ï¼‰
        fact = query_text.replace("è¯·è®°ä½", "").replace("è®°ä½", "").strip("ï¼š:ï¼Œ,ã€‚. ")
        if fact:
            updated_user_facts.append(fact)
            # No chat history update here, that's for the UI layer.
            return "", updated_user_facts # Indicate it's a memory command, no query for RAG

    # æ‹¼æ¥æ‰€æœ‰è®°å¿†å†…å®¹åˆ°ç”¨æˆ·è¾“å…¥å‰é¢
    if updated_user_facts: #è¿™éƒ¨åˆ†çš„memory_prefixéƒ½æ˜¯ç”¨æˆ·è¦æ±‚è®°ä½çš„å†…å®¹ï¼Œå¯ä»¥ç†è§£ä¸ºé—®é¢˜é‡å†™äº†ä¸€éï¼Œä½†è‡³å°‘å¢åŠ äº†ä¸€éƒ¨åˆ†è¦æ±‚è®°ä½çš„å†…å®¹
        memory_prefix = "ï¼Œ".join(updated_user_facts)
        full_query = f"è¯·è®°ä½ï¼š{memory_prefix}ã€‚ç”¨æˆ·æé—®ï¼š{query_text}"
    else:
        full_query = query_text

    return full_query, updated_user_facts

# 1. å®šä¹‰æ–‡æ¡£åŠ è½½å‡½æ•°ï¼Œæ”¯æŒPDFå’ŒWord ä»¥åŠè¿”å›æ–‡æ¡£å†…å®¹åˆ—è¡¨
def load_documents(directory_path):
    documents = []

    for file in os.listdir(directory_path):
        file_path = os.path.join(directory_path, file)

        try:
            if file.endswith('.pdf'): #å°†pdfæ–‡ä»¶è§£æä¸ºå¤šä¸ªç‰‡æ®µï¼Œç„¶åå°†è¿™äº›ç‰‡æ®µæ”¾å…¥å¤§çš„documentä¸­
                loader = PyPDFLoader(file_path) #è¿™æ˜¯ä¸€ä¸ªåŠ è½½å™¨ï¼Œè¯»å–pdfæ–‡ä»¶ï¼Œè½¬åŒ–ä¸ºæ–‡æ¡£ç‰‡æ®µï¼ˆdocument chunksï¼‰
                documents.extend(loader.load()) #loader.load()è¿”å›ä¸€ä¸ªåˆ—è¡¨ éƒ½æ˜¯documentå¯¹è±¡   ç„¶åé€šè¿‡extendå°†æ‰€æœ‰çš„å¯¹è±¡æ”¾åœ¨documentä¸­ 
            elif file.endswith('.docx') or file.endswith('.doc'):
                loader = Docx2txtLoader(file_path)
                documents.extend(loader.load())
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½æ–‡ä»¶ {file}ï¼Œè·³è¿‡æ­¤æ–‡ä»¶ã€‚é”™è¯¯ï¼š{e}")
            continue

    return documents

def split_documents(documents: list) -> list:
    """
    åªä¼ å…¥ documents åˆ—è¡¨ï¼Œå¯¹æ¯ä¸ªæ–‡æ¡£å…ˆæŒ‰ç« èŠ‚åˆ†ï¼Œå†æŒ‰å—åˆ†
    å†…éƒ¨ä½¿ç”¨é»˜è®¤è§„åˆ™ï¼ˆåˆ†ç« èŠ‚è§„åˆ™ + chunk_size + chunk_overlapï¼‰
    è¿”å›æ‰€æœ‰æ‹†åˆ†åçš„ Document å¯¹è±¡
    """

    # å›ºå®šå‚æ•°
    section_pattern = r"\n(?=\d{1,2}\s)"  # å¦‚ 1 èŒƒå›´ã€2 å¼•ç”¨æ–‡ä»¶ ç« èŠ‚åˆ’åˆ†çš„æ ¼å¼å¯ä»¥æ›´åŠ å®Œå–„ä¸€äº› ç›®å‰çš„æ­£åˆ™åŒ¹é…æ˜¯ æ¢è¡Œ+æ•°å­—+ç©ºæ ¼ (?=)æ˜¯æ­£å‘é¢„æŸ¥

    # æ‹†åˆ†å™¨é…ç½®
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", ". ", " ", ""], #æ„Ÿè§‰é—®å· æ„Ÿå¹å·ä»€ä¹ˆçš„è¿˜æ˜¯æ”¾ç½®ä¸€èµ·å¥½ 
        is_separator_regex=False
    )

    all_chunks = []

    for doc in documents:
        text = doc.page_content
        metadata = doc.metadata or {}

        # å…ˆæŒ‰ç« èŠ‚æ­£åˆ™åˆ†
        sections = re.split(section_pattern, text)

        for section in sections:
            cleaned = section.strip()
            if not cleaned:
                continue

            # çŸ­ç« èŠ‚ç›´æ¥ä¿ç•™
            if len(cleaned) <= CHUNK_SIZE:
                all_chunks.append(Document(page_content=cleaned, metadata=metadata))
            else:
                # é•¿ç« èŠ‚å†æ‹†å—
                sub_chunks = splitter.split_text(cleaned)
                for chunk in sub_chunks:
                    all_chunks.append(Document(page_content=chunk, metadata=metadata))

    return all_chunks

# 2. æ–‡æœ¬åˆ†å‰²  åˆ›å»ºå‡ºé€‚åˆåµŒå…¥æ¨¡å‹çš„å°æ–‡æœ¬å—  æŒ‰ç…§åˆ†éš”ç¬¦åˆ‡å‰²ï¼Œæ¯å—é•¿åº¦ä¸è¶…è¿‡chunk_size å…è®¸ç›¸é‚»çš„æœ‰chunk_overlapçš„å­—ç¬¦é‡å  æœ€ç»ˆè¿”å›åˆ‡å¥½çš„å°å—åˆ—è¡¨
# def split_documents(documents):
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=CHUNK_SIZE,
#         chunk_overlap=CHUNK_OVERLAP,
#         length_function=len, #è¿™ä¸ªlenä¸æ˜¯å˜é‡ è€Œæ˜¯å°†lenå‡½æ•°  ä¼ å…¥
#         separators=[  #æ‹†åˆ†çš„é€»è¾‘é¡ºåº
#             "\n\n",  # æŒ‰åŒæ¢è¡Œåˆ†æ®µï¼ˆæ®µè½ä¼˜å…ˆï¼‰
#             "\n",    # æŒ‰å•æ¢è¡Œåˆ†æ®µ
#             ". ",    # æŒ‰è‹±æ–‡å¥å·+ç©ºæ ¼åˆ†
#             "? ",    # æŒ‰é—®å·+ç©ºæ ¼åˆ†
#             "! ",    # æŒ‰æ„Ÿå¹å·+ç©ºæ ¼åˆ†
#             "ã€‚ ",   # ä¸­æ–‡å¥å·+ç©ºæ ¼
#             "ï¼Ÿ ",   # ä¸­æ–‡é—®å·+ç©ºæ ¼
#             "ï¼ ",   # ä¸­æ–‡æ„Ÿå¹å·+ç©ºæ ¼
#             "ã€‚\n",  # ä¸­æ–‡å¥å·+æ¢è¡Œ
#             "ï¼Ÿ\n",  # ä¸­æ–‡é—®å·+æ¢è¡Œ
#             "ï¼\n",  # ä¸­æ–‡æ„Ÿå¹å·+æ¢è¡Œ
#             " ",     # æŒ‰ç©ºæ ¼åˆ†ï¼ˆå¦‚æœè¿˜æ²¡åˆ†å®Œï¼‰
#             ""       # æœ€åæŒ‰å­—ç¬¦åˆ†ï¼ˆæœ€ç»†ç²’åº¦ï¼‰
#         ],
#         is_separator_regex=False
#     )
#     return text_splitter.split_documents(documents)

# 3. åˆå§‹åŒ–HuggingFaceåµŒå…¥æ¨¡å‹ é…ç½®gpuåŠ é€Ÿ  è¿”å›æ–‡æœ¬å‘é‡åŒ–å™¨ 
def initialize_embeddings():
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_PATH,
        model_kwargs={'device': EMBEDDING_DEVICE}
    )

# 4. åˆ›å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“ (Modified) æ£€æµ‹æ•°æ®åº“çŠ¶æ€  å¤„ç†æ¨¡å‹å˜æ›´å¯¼è‡´çš„ç»´åº¦é—®é¢˜ æ”¯æŒå¢é‡æ›´æ–°æ–‡æ¡£
def get_vector_db(chunks, embeddings, persist_directory): #è¿™ä¸ªå‡½æ•°è¡¨æ˜å¦‚æœæœ‰å‘é‡æ•°æ®åº“åˆ™ç›´æ¥è¯»å–å‘é‡æ•°æ®åº“ç„¶åè¿”å›å³å¯ï¼Œé‚£ä¹ˆå¦‚æœæœ‰æ–°å¢çš„æ–‡ä»¶ï¼Œè¿™ä¸ªå‡½æ•°æ²¡åŠæ³•å¤„ç†
    """Creates a new vector DB or loads an existing one."""
    if os.path.exists(persist_directory) and os.listdir(persist_directory): #æœ‰ç°æˆæ•°æ®åº“å°±åŠ åœ¨ï¼Œæ²¡æœ‰å°±çœ‹æœ‰æ²¡æœ‰chunkï¼Œæœ‰å°±åˆ›å»º 
        print(f"Loading existing vector database from {persist_directory}...")
        try:
            # When loading, ChromaDB will check for dimension compatibility.
            # If EMBEDDING_MODEL_PATH changed leading to a dimension mismatch, this will fail.
            return Chroma(persist_directory=persist_directory, embedding_function=embeddings) #chromaä¼šä»ç›®å½•ä¸­æ‰¾æ•°æ®æ–‡ä»¶  å¹¶ä¸”å¯ç”¨embedding_functionå»é…ç½®
        except Exception as e: #ç¡®ä¿embeddingæ¨¡å‹å¯¹çš„
            print(f"Error loading existing vector database: {e}.")
            print(f"This might be due to a change in the embedding model and a dimension mismatch.")
            print(f"If you changed EMBEDDING_MODEL_PATH, you MUST delete the old database directory: {persist_directory}")
            # If loading fails, proceed as if it doesn\'t exist, but only create if chunks are given later.
            return None # Indicate loading failed or DB doesn't exist in a usable state
    else:
        # Directory doesn't exist or is empty
        if chunks:
            print(f"Creating new vector database in {persist_directory}...")
            print(f"Creating Chroma DB with {len(chunks)} chunks...")
            try:
                vector_db_new = Chroma.from_documents(
                    documents=chunks,
                    embedding=embeddings,
                    persist_directory=persist_directory
                )
                print("Vector database created and persisted.")
                return vector_db_new
            except Exception as e:
                print(f"Error creating new vector database: {e}")
                raise  # ç»§ç»­æŠ›å‡ºåŸå§‹å¼‚å¸¸
        else:
            # No chunks provided and DB doesn't exist/is empty - cannot create.
            print(f"Vector database directory {persist_directory} not found or empty, and no chunks provided to create a new one.")
            return None # Indicate DB doesn't exist and cannot be created yet

# 5. åˆå§‹åŒ–è¿æ¥åˆ°VLLMæœåŠ¡å™¨çš„ChatOpenAIå®¢æˆ·ç«¯ (Replaces initialize_llm) è¿æ¥VLLMæ¨ç†æœåŠ¡å™¨ é…ç½®æ¨¡å‹ è¿”å›å…¼å®¹æ¥å£  è¿™ä¸€æ­¥ä¹Ÿç›¸å½“äºç»™å…¨å±€å˜é‡llmèµ‹å€¼
def initialize_openai_client():
    """Initializes ChatOpenAI client pointing to the VLLM server."""
    print(f"Initializing ChatOpenAI client for VLLM server at {VLLM_BASE_URL}...")
    return ChatOpenAI(
        openai_api_base=VLLM_BASE_URL,
        openai_api_key=VLLM_API_KEY,
        model_name=VLLM_MODEL_NAME
    )

def create_rag_chain_with_memory(vector_db_arg, llm_arg, memory_arg): #åˆ›å»ºå¸¦æœ‰è®°å¿†çš„é—®ç­”é“¾  è¿™éƒ¨åˆ†çš„æç¤ºè¯ç›¸å½“äºéƒ½æ˜¯å°è£…çš„ ç”¨åˆ«äººçš„åº“ å¯ä»¥å°è¯•è‡ªå·±æ‰‹å†™
    retriever = vector_db_arg.as_retriever(search_kwargs={"k": SEARCH_K}) #å°†å‘é‡æ•°æ®åº“å˜ä¸ºä¸€ä¸ªæ£€ç´¢å™¨ï¼ˆåŒ…è£…æˆï¼ŒåŒæ—¶ä¹Ÿè®¾ç½®å¥½äº†æ¥å£ï¼‰ï¼Œèƒ½è¾“å…¥é—®é¢˜è¿”å›æœ€ç›¸å…³çš„kä¸ªæ–‡æœ¬ 
    # ç›´æ¥ç”¨ ConversationalRetrievalChainï¼Œè‡ªåŠ¨ç®¡ç†ä¸Šä¸‹æ–‡
    return ConversationalRetrievalChain.from_llm( #å°è£…çš„ç±»ï¼Œ å¯ä»¥æŠŠæ£€ç´¢å™¨è¿”å›çš„æ–‡æœ¬ï¼Œå½“å‰å¯¹è¯ å†å²å¯¹è¯æ‹¼æ¥æˆä¸€ä¸ªprompt   ï¼ˆæ£€ç´¢å‘é‡åº“å¾—åˆ°æœ€ç›¸å…³çš„kä¸ªï¼Œè·å–memory æ‹¼æ¥prompt LLMè°ƒç”¨è¿”å›ç»“æœï¼Œåˆ†åˆ«å¯¹åº”ä¸‹é¢çš„å‚æ•°ï¼‰
        llm=llm_arg, #è¿™ä¸ªæŒ‡ç”¨å“ªä¸ªæ¨¡å‹å›ç­”
        retriever=retriever, #è¿™æ˜¯æŒ‡ç”¨å“ªä¸ªæ£€ç´¢å™¨ 
        memory=memory_arg,
        return_source_documents=False  # å¦‚æœä¸éœ€è¦è¾“å‡ºæ£€ç´¢åˆ°çš„æºæ–‡æ¡£ï¼Œå¯è®¾ä¸º False 
    )

# 7. Function to process query using the RAG chain (Modified for Streaming)
def process_query(query):
    """Processes a user query using the RAG chain and streams the answer."""
    global rag_chain, vector_db # Add vector_db to globals accessed here for debugging  è¿™é‡Œæ˜¯è¡¨æ˜ è¿™ä¸¤ä¸ªæ˜¯å…¨å±€å˜é‡ å¯ä»¥è®©å‡½æ•°å†…éƒ¨ä¿®æ”¹å‡½æ•°å¤–éƒ¨å®šä¹‰çš„å˜é‡ï¼ˆå¦‚æœæ²¡å®šä¹‰è¿™ä¸ªï¼Œå‡½æ•°å†…æ˜¯ä¸èƒ½ä¿®æ”¹å…¨å±€å˜é‡ï¼‰ ä¹Ÿå°±æ˜¯ä¹‹å‰å®šä¹‰è¿‡çš„ï¼Œå¦‚æœä¸å£°æ˜ ä¼šå‡ºé”™  
    # åœ¨æ¯æ¬¡å¤„ç†å‰è£å‰ªè®°å¿†
    trim_memory_tokens(memory)
    if rag_chain is None:
        yield "é”™è¯¯ï¼šRAG é“¾æœªåˆå§‹åŒ–ã€‚"
        return

    # --- For Debugging Retrieval ---
    # Uncomment the block below to see what documents are retrieved by the vector DB
    if vector_db:
        try:
            retrieved_docs = vector_db.similarity_search(query, k=SEARCH_K) #æ£€ç´¢æœ€ç›¸è¿‘çš„æ–‡æ¡£
            print(f"\n--- Retrieved Documents for query: '{query}' ---")
            for i, doc in enumerate(retrieved_docs):
                # Attempt to get score if retriever supports it (Chroma's similarity_search_with_score)
                # For basic similarity_search, score might not be directly in metadata.
                # If using retriever.get_relevant_documents(), score might be present.
                score = doc.metadata.get('score', 'N/A') # å…ˆä»metadataä¸­å–score æ²¡æœ‰å°±è¿”å›NA
                if hasattr(doc, 'score'): #  æœ‰äº›documentæ˜¯å¸¦scoreå±æ€§  è¿™ä¸ªç›¸å½“äºæ˜¯ä¸€ç§å…¼å®¹æ€§ å¦‚æœå‰è€…æ²¡æ‰¾åˆ° å°±æ‰¾è¿™ä¸ª
                    score = doc.score
                
                print(f"Doc {i+1} (Score: {score}):")
                print(f"Content: {doc.page_content[:500]}...") # Print first 500 chars è¡¨ç¤ºåªæ‰“å°å‰äº”ç™¾å­—ç¬¦ è¿™æ ·å¯é¿å…ç»ˆç«¯å†…å®¹è¿‡å¤š
                print(f"Metadata: {doc.metadata}")
            print("--- End Retrieved Documents ---\n")
        except Exception as e:
            print(f"Error during debug similarity_search: {e}")
    else:
        print("Vector DB not initialized, skipping debug retrieval.")
    # --- End Debugging Retrieval ---

    try:
        print(f"å¼€å§‹å¤„ç†æµå¼æŸ¥è¯¢: {query}")

        # Directly stream from the RAG chain runnable
        # The input format for create_retrieval_chain is typically {"input": query}
        # The output chunks often contain 'answer' and 'context' keys
        # response_stream = rag_chain.stream({"input": query})
        response_stream = rag_chain.stream({ #ä¼ å…¥å­—å…¸ ç„¶åæµå¼å›ç­”  #è¿™ä¸ªresponse_streamæ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡
                "question": query,
                                            })

        full_answer = ""
        # Yield chunks as they arrive. Gradio Textbox updates incrementally.
        print("å¼€å§‹æµå¼ç”Ÿæˆå›ç­”...")
        for chunk in response_stream: #æ¯å¾ªç¯ä¸€æ¬¡éƒ½ä¼šæ‹¿åˆ°æ–°çš„å†…å®¹ æˆ‘è§‰å¾—è¿™é‡Œæ‰æ˜¯æµå¼è¾“å‡ºçš„å…³é”® ä¹‹å‰çš„yieldéƒ½ä¸ç®—
            # Check if the 'answer' key exists in the chunk and append it
            answer_part = chunk.get("answer", "") #get è¿™ä¸ªé”®å€¼ æ²¡æœ‰åˆ™è¿”å›ç©º
            if answer_part:
                full_answer += answer_part 
                # Debugging output
                # print(f"Raw answer_part from LLM: '{answer_part}'")
                # print(f"Yielding to Gradio: '{full_answer}'")
                yield full_answer # Yield the progressively built answer

        if not full_answer:
             yield "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›ç­”ã€‚" # Handle cases where stream completes without answer

        print(f"æµå¼å¤„ç†å®Œæˆã€‚æœ€ç»ˆå›ç­”: {full_answer}")

    except Exception as e:
        print(f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc() # Print stack trace for debugging
        yield f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {e}"

def safe_add_documents(vector_db, chunks, max_batch_size=5000): 
    """
    å®‰å…¨åˆ†æ‰¹æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
    
    å‚æ•°:
        vector_db: å·²åˆå§‹åŒ–çš„å‘é‡æ•°æ®åº“å¯¹è±¡
        chunks: å¾…æ·»åŠ çš„æ–‡æ¡£å—åˆ—è¡¨
        max_batch_size: å•æ¬¡æ‰¹é‡ä¸Šé™
    """
    total_chunks = len(chunks)
    
    for batch_start in range(0, total_chunks, max_batch_size):
        batch = chunks[batch_start : batch_start + max_batch_size]
        batch_num = (batch_start // max_batch_size) + 1
        
        try:
            print(f"ğŸ”„ æ­£åœ¨æ·»åŠ ç¬¬ {batch_num} æ‰¹ï¼ˆ{len(batch)} ä¸ªchunkï¼‰...")
            vector_db.add_documents(batch)
            print(f"âœ… ç¬¬ {batch_num} æ‰¹æ·»åŠ æˆåŠŸ")
        except Exception as e:
           
            print(f"âŒ ç¬¬ {batch_num} æ‰¹æ·»åŠ å¤±è´¥ï¼ˆæœ€ç»ˆå°è¯•ï¼‰ï¼š{str(e)}")
            raise  # æŠ›å‡ºå¼‚å¸¸ç»ˆæ­¢ç¨‹åº

# 8. Function to rebuild the index and RAG chain (Modified to add documents)
def rebuild_index_and_chain(): #è¿™ä¸ªå‡½æ•°çš„é€»è¾‘æœ‰é—®é¢˜  åæœŸéœ€è¦ä¿®å¤
    """Loads documents, creates/updates vector DB by adding new content, and rebuilds the RAG chain."""
    global vector_db, rag_chain, embeddings, llm


    if embeddings is None or llm is None:
        return "é”™è¯¯ï¼šEmbeddings æˆ– LLM æœªåˆå§‹åŒ–ã€‚"

    # Ensure documents directory exists
    if not os.path.exists(DOCUMENTS_DIR):
        os.makedirs(DOCUMENTS_DIR)
        print(f"åˆ›å»ºæ–‡æ¡£ç›®å½•: {DOCUMENTS_DIR}")

    # Step 1: Load documents
    print("åŠ è½½æ–‡æ¡£...")
    documents = load_documents(DOCUMENTS_DIR) #è¿™éƒ¨åˆ†é€»è¾‘ä¸å¤ªå¯¹ å› ä¸ºè¿™ä¸ªç›®å½•åŒ…æ‹¬äº†ä¹‹å‰çš„æ–‡æ¡£ é‚£ä¹ˆä¼šå¯¼è‡´æ–‡æ¡£çš„é‡å¤æ·»åŠ  ä½†æ˜¯è¿™æ˜¯åœ¨æœªæ‰¾åˆ°æ–‡æ¡£çš„æƒ…å†µæ‰åŠ è½½ä¹‹å‰çš„å‘é‡æ•°æ®åº“ ä¹Ÿè¿˜å¥½ï¼Œä»¥ååˆ æ‰æ·»åŠ æ–‡æ¡£çš„åŠŸèƒ½å°±å¯ä»¥ï¼Œæˆ–è€…å¢åŠ ä¸€æ¡æ·»åŠ æ–‡æ¡£åä¼šè¿›è¡Œç­›æŸ¥
    if not documents: 
        print(f"åœ¨ {DOCUMENTS_DIR} ä¸­æœªæ‰¾åˆ°æ–‡æ¡£ã€‚")
        # Try to load existing DB even if no new documents are found
        print("å°è¯•åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
        # Pass None for chunks as we are just trying to load
        vector_db_loaded = get_vector_db(None, embeddings, PERSIST_DIR)
        if vector_db_loaded:
            vector_db = vector_db_loaded
            print("æ²¡æœ‰æ–°æ–‡æ¡£åŠ è½½ï¼Œå°†ä½¿ç”¨ç°æœ‰çš„å‘é‡æ•°æ®åº“ã€‚é‡æ–°åˆ›å»º RAG é“¾...")
            rag_chain = create_rag_chain_with_memory(vector_db, llm, memory)

            return "æ²¡æœ‰æ‰¾åˆ°æ–°æ–‡æ¡£ï¼Œå·²ä½¿ç”¨ç°æœ‰æ•°æ®é‡æ–°åŠ è½½ RAG é“¾ã€‚"
        else:
            # No documents AND no existing DB
            return "é”™è¯¯ï¼šæ²¡æœ‰æ–‡æ¡£å¯åŠ è½½ï¼Œä¸”æ²¡æœ‰ç°æœ‰çš„å‘é‡æ•°æ®åº“ã€‚"

    # Step 2: Split text
    print("åˆ†å‰²æ–‡æœ¬...") 
    chunks = split_documents(documents)
    # for chunk in chunks:
    #     print("chunksæ˜¯ä»€ä¹ˆ\n",chunk)
    if not chunks:
        print("åˆ†å‰²åæœªç”Ÿæˆæ–‡æœ¬å—ã€‚")
        # Try loading existing DB if splitting yielded nothing
        print("å°è¯•åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...") 
        vector_db_loaded = get_vector_db(None, embeddings, PERSIST_DIR)
        if vector_db_loaded:
             vector_db = vector_db_loaded
             print("è­¦å‘Šï¼šæ–°åŠ è½½çš„æ–‡æ¡£åˆ†å‰²åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ã€‚ä½¿ç”¨ç°æœ‰æ•°æ®åº“ã€‚")
             rag_chain = create_rag_chain_with_memory(vector_db, llm, memory)# Ensure chain is recreated
             return "è­¦å‘Šï¼šæ–‡æ¡£åˆ†å‰²åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ã€‚RAG é“¾å·²ä½¿ç”¨ç°æœ‰æ•°æ®é‡æ–°åŠ è½½ã€‚"
        else:
            # No chunks AND no existing DB
            return "é”™è¯¯ï¼šæ–‡æ¡£åˆ†å‰²åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ï¼Œä¸”æ— ç°æœ‰æ•°æ®åº“ã€‚"

    # Step 3: Load or Create/Update vector database
    print("åŠ è½½æˆ–æ›´æ–°å‘é‡æ•°æ®åº“...") #åŠ è½½æ–‡æ¡£ä¸­çš„å‘é‡æ•°æ®åº“ ä½†å¦‚æœæ–‡æ¡£å·²ç»è½¬æ¢è¿‡å‘é‡æ•°æ®åº“ é‚£ä¹ˆä¼šå¯¼è‡´å†æ·»åŠ ä¸€æ¬¡  é™¤éè®¾ç½®å¥½è¿™é‡Œçš„æ–‡æ¡£éƒ½æ˜¯æ–°æ·»åŠ è¿‡çš„æ–‡æ¡£
    # Try loading first, even if we have chunks (in case we want to add to it)
    vector_db_loaded = get_vector_db(None, embeddings, PERSIST_DIR)

    if vector_db_loaded:
        print(f"å‘ç°æœ‰å‘é‡æ•°æ®åº“æ·»åŠ  {len(chunks)} ä¸ªå—...") #ä¸»è¦é—®é¢˜åœ¨äºè¿™ä¸ªchunkå¯èƒ½åŒ…å«é‡å¤å†…å®¹
        vector_db = vector_db_loaded # Use the loaded DB
        try:
            safe_add_documents(vector_db, chunks, max_batch_size=5000)
            #vector_db.add_documents(chunks) #æ³¨æ„ è¿™ä¸ªä¸Šä¼ æœ‰é™åˆ¶ï¼Œä¸€æ¬¡æ€§çš„chunksä¸èƒ½å¤ªå¤š ä¸èƒ½è¶…è¿‡äº”åƒ
            #print("å—æ·»åŠ æˆåŠŸã€‚")
            # Persisting might be needed depending on Chroma version/setup, often automatic.
            # vector_db.persist() # Uncomment if persistence issues occur
        except Exception as e:
            print(f"æ·»åŠ æ–‡æ¡£åˆ° Chroma æ—¶å‡ºé”™: {e}")
            print("ä½¿ç”¨äºŒåˆ†æ³•é€’å½’å®šä½å‡ºé”™æ–‡æ¡£...")
            # äºŒåˆ†æ³•é€’å½’å®šä½å‡ºé”™chunk
            def bisect_add(chunks, add_func):
                if not chunks:
                    return
                try:
                    add_func(chunks)
                except Exception as e_inner:
                    if len(chunks) == 1:
                        chunk = chunks[0]
                        doc_name = getattr(chunk, 'metadata', {}).get('source', None) or str(chunk)[:100]
                        print(f"å‡ºé”™chunk: {doc_name}, é”™è¯¯: {e_inner}")
                    else:
                        mid = len(chunks) // 2
                        bisect_add(chunks[:mid], add_func)
                        bisect_add(chunks[mid:], add_func)
            bisect_add(chunks, lambda cs: vector_db.add_documents(cs)) #lambda cs: vector_db.add_documents(cs) å®é™…æ·»åŠ æ“ä½œçš„lambdaå‡½æ•°
            rag_chain = create_rag_chain_with_memory(vector_db, llm, memory)
            return f"é”™è¯¯ï¼šå‘å‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£æ—¶å‡ºé”™: {e}ã€‚RAGé“¾å¯èƒ½ä½¿ç”¨æ—§æ•°æ®ã€‚"
    else:
        # Database didn't exist or couldn't be loaded, create a new one with the current chunks
        print(f"åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“å¹¶æ·»åŠ  {len(chunks)} ä¸ªå—...")
        try:
            # Call get_vector_db again, this time *with* chunks to trigger creation
            newly_created_vector_db = get_vector_db(chunks, embeddings, PERSIST_DIR) #ç”¨äºæ–°å»ºæ•°æ®åº“
            if newly_created_vector_db is None:
                 raise RuntimeError("get_vector_db failed to create a new database.")
            vector_db = newly_created_vector_db # Assign the newly created DB to the global variable
            print("æ–°çš„å‘é‡æ•°æ®åº“å·²åˆ›å»ºå¹¶æŒä¹…åŒ–ã€‚")
        except Exception as e:
            print(f"åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            return f"é”™è¯¯ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}"

    if vector_db is None:
         # This should ideally not be reached if error handling above is correct
         return "é”™è¯¯ï¼šæœªèƒ½åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“ã€‚"

    # Step 4: Create RAG chain
    print("åˆ›å»º RAG é“¾...")
    rag_chain = create_rag_chain_with_memory(vector_db, llm, memory)
    print("ç´¢å¼•å’Œ RAG é“¾å·²æˆåŠŸæ›´æ–°ã€‚")
    return "æ–‡æ¡£å¤„ç†å®Œæˆï¼Œç´¢å¼•å’Œ RAG é“¾å·²æ›´æ–°ã€‚"


# Helper function to list documents in the directory ç”Ÿæˆå·²åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨ markdownæ ¼å¼åŒ–è¾“å‡º  å®æ—¶æ›´æ–°æ–‡æ¡£çŠ¶æ€
# def get_loaded_documents_list():
#     """Returns a Markdown formatted list of files in DOCUMENTS_DIR."""
#     if not os.path.exists(DOCUMENTS_DIR) or not os.listdir(DOCUMENTS_DIR):
#         return "å½“å‰æ²¡æœ‰å·²åŠ è½½çš„æ–‡æ¡£ã€‚"
#     try:
#         files = [f for f in os.listdir(DOCUMENTS_DIR) if os.path.isfile(os.path.join(DOCUMENTS_DIR, f)) and (f.endswith('.pdf') or f.endswith('.docx') or f.endswith('.doc'))]
#         if not files: 
#             return "å½“å‰æ²¡æœ‰å·²åŠ è½½çš„æ–‡æ¡£ã€‚"
#         markdown_list = "### å½“å‰å·²åŠ è½½æ–‡æ¡£:\n" + "\n".join([f"- {file}" for file in files])
#         return markdown_list
#     except Exception as e:
#         print(f"Error listing documents: {e}")
#         return "æ— æ³•åˆ—å‡ºæ–‡æ¡£ã€‚"

#è¿™ä¸ªåŠŸèƒ½åº”è¯¥ä¸éœ€è¦
# 9. Function to handle file uploads (Modified to return doc list) 
# def handle_file_upload(file_obj):
#     """Saves the uploaded file, triggers index rebuilding, and returns status and doc list."""
#     if file_obj is None:
#         return "æœªé€‰æ‹©æ–‡ä»¶ã€‚", get_loaded_documents_list() # Return current list even if no file selected

#     try:
#         # Gradio provides a temporary file path
#         temp_file_path = file_obj.name
#         file_name = os.path.basename(temp_file_path)
#         destination_path = os.path.join(DOCUMENTS_DIR, file_name)

#         print(f"å°†ä¸Šä¼ çš„æ–‡ä»¶ä» {temp_file_path} å¤åˆ¶åˆ° {destination_path}")
#         # Ensure documents directory exists
#         if not os.path.exists(DOCUMENTS_DIR):
#             os.makedirs(DOCUMENTS_DIR)
#         shutil.copy(temp_file_path, destination_path) # Copy the file

#         print(f"æ–‡ä»¶ {file_name} ä¸Šä¼ æˆåŠŸã€‚å¼€å§‹é‡å»ºç´¢å¼•...")
#         status = rebuild_index_and_chain()
#         final_status = f"æ–‡ä»¶ '{file_name}' ä¸Šä¼ æˆåŠŸã€‚\n{status}"
#         # Get updated document list
#         doc_list_md = get_loaded_documents_list()
#         return final_status, doc_list_md

#     except Exception as e:
#         print(f"æ–‡ä»¶ä¸Šä¼ æˆ–å¤„ç†å¤±è´¥: {e}")
#         # Return error and current doc list
#         return f"æ–‡ä»¶ä¸Šä¼ æˆ–å¤„ç†å¤±è´¥: {e}", get_loaded_documents_list() 


# def detect_and_remove_duplicates():
#     """
#     æ£€æµ‹å¹¶æ¸…é™¤å‘é‡åº“ä¸­çš„é‡å¤å†…å®¹
#     è¿”å›æ£€æµ‹ç»“æœå’Œæ¸…ç†çŠ¶æ€
#     """
#     global vector_db
    
#     if vector_db is None:
#         return "é”™è¯¯ï¼šå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–"
    
#     try:
#         print("å¼€å§‹æ£€æµ‹å‘é‡åº“ä¸­çš„é‡å¤å†…å®¹...")
        
#         # è·å–å‘é‡åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£
#         all_docs = vector_db.get()
#         if not all_docs or not all_docs['documents']:
#             return "å‘é‡åº“ä¸ºç©ºï¼Œæ— éœ€æ£€æµ‹é‡å¤å†…å®¹"
        
#         documents = all_docs['documents']
#         metadatas = all_docs['metadatas']
#         ids = all_docs['ids']
        
#         print(f"å‘é‡åº“ä¸­å…±æœ‰ {len(documents)} ä¸ªæ–‡æ¡£å—")
        
#         # æ£€æµ‹é‡å¤å†…å®¹
#         seen_contents = {}
#         duplicate_ids = []
#         duplicate_info = []
        
#         for i, (doc_id, content, metadata) in enumerate(zip(ids, documents, metadatas)):
#             # æ¸…ç†å†…å®¹ç”¨äºæ¯”è¾ƒï¼ˆå»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œï¼‰
#             cleaned_content = ' '.join(content.strip().split())
            
#             if cleaned_content in seen_contents:
#                 # å‘ç°é‡å¤
#                 original_id = seen_contents[cleaned_content]
#                 duplicate_ids.append(doc_id)
#                 duplicate_info.append({
#                     'duplicate_id': doc_id,
#                     'original_id': original_id,
#                     'content_preview': content[:100] + '...' if len(content) > 100 else content,
#                     'source': metadata.get('source', 'unknown') if metadata else 'unknown'
#                 })
#                 print(f"å‘ç°é‡å¤å†…å®¹ - ID: {doc_id}, åŸå§‹ID: {original_id}")
#             else:
#                 seen_contents[cleaned_content] = doc_id
        
#         if not duplicate_ids:
#             return "æœªå‘ç°é‡å¤å†…å®¹"
        
#         # åˆ é™¤é‡å¤å†…å®¹
#         print(f"å¼€å§‹åˆ é™¤ {len(duplicate_ids)} ä¸ªé‡å¤æ–‡æ¡£...")
#         vector_db.delete(ids=duplicate_ids)
        
#         # é‡æ–°åˆ›å»ºRAGé“¾
#         global rag_chain, llm, memory
#         if llm is not None:
#             rag_chain = create_rag_chain_with_memory(vector_db, llm, memory)
        
#         # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
#         report = f"é‡å¤å†…å®¹æ¸…ç†å®Œæˆï¼\n"
#         report += f"- æ£€æµ‹åˆ° {len(duplicate_ids)} ä¸ªé‡å¤æ–‡æ¡£\n"
#         report += f"- å·²åˆ é™¤é‡å¤å†…å®¹ï¼Œä¿ç•™åŸå§‹æ–‡æ¡£\n"
#         report += f"- å½“å‰å‘é‡åº“å‰©ä½™ {len(documents) - len(duplicate_ids)} ä¸ªæ–‡æ¡£å—\n\n"
        
#         report += "é‡å¤å†…å®¹è¯¦æƒ…ï¼š\n"
#         for i, dup in enumerate(duplicate_info, 1):
#             report += f"{i}. é‡å¤ID: {dup['duplicate_id']}\n"
#             report += f"   åŸå§‹ID: {dup['original_id']}\n"
#             report += f"   æ¥æº: {dup['source']}\n"
#             report += f"   å†…å®¹é¢„è§ˆ: {dup['content_preview']}\n\n"
        
#         print("é‡å¤å†…å®¹æ¸…ç†å®Œæˆ")
#         return report
        
#     except Exception as e:
#         error_msg = f"æ£€æµ‹é‡å¤å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {e}"
#         print(error_msg)
#         import traceback
#         traceback.print_exc()
#         return error_msg

#ä¸éœ€è¦è¿™ä¸ªåŠŸèƒ½ åœ¨æ¯æ¬¡è¿è¡Œçš„æ—¶å€™ æå‰å°†æ‰€æœ‰æ–‡æ¡£ä¼ å…¥ä¸Šå» æ¯æ¬¡æ¸…ç©ºå‘é‡æ•°æ®åº“,è¿™æ ·å°±ä¸éœ€è¦å¯¹äºå‘é‡æ•°æ®åº“æ“ä½œçš„å‡½æ•°
# def analyze_vector_db_content():
#     """
#     åˆ†æå‘é‡åº“å†…å®¹ï¼Œæä¾›ç»Ÿè®¡ä¿¡æ¯
#     """
#     global vector_db
    
#     if vector_db is None:
#         return "é”™è¯¯ï¼šå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–"
    
#     try:
#         # è·å–å‘é‡åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£
#         all_docs = vector_db.get()
#         if not all_docs or not all_docs['documents']:
#             return "å‘é‡åº“ä¸ºç©º"
        
#         documents = all_docs['documents']
#         metadatas = all_docs['metadatas']
#         ids = all_docs['ids']
        
#         # ç»Ÿè®¡ä¿¡æ¯
#         total_chunks = len(documents)
#         total_chars = sum(len(doc) for doc in documents)
#         avg_chunk_length = total_chars / total_chunks if total_chunks > 0 else 0
        
#         # æŒ‰æ¥æºæ–‡ä»¶ç»Ÿè®¡
#         source_stats = {}
#         for metadata in metadatas:
#             if metadata and 'source' in metadata:#æ£€æŸ¥metadataä¸­æœ‰æ²¡æœ‰sourceè¿™ä¸ªé”® è¿™ä¸ªsourceä»£è¡¨æ•°æ®æ¥è‡ªå“ªä¸ªæ–‡ä»¶
#                 source = metadata['source']
#                 source_stats[source] = source_stats.get(source, 0) + 1 #source_stats.get(source, 0)è¿™ä¸ªæ˜¯source_statsä¸­sourceå¯¹åº”çš„å€¼ å¦‚æœæ²¡æœ‰åˆ™è¿”å›0
        
#         # æ£€æµ‹æ½œåœ¨é‡å¤ 
#         content_hashes = {}
#         potential_duplicates = 0
        
#         for content in documents:
#             cleaned_content = ' '.join(content.strip().split()) #stripæ¶ˆé™¤å‰åç©ºæ ¼å’Œåˆ†è¡Œç¬¦ï¼Œç„¶åé€šè¿‡splitå°†ä¸€ä¸ªå­—ç¬¦ä¸²åˆ†ä¸ºå¤šä¸ªå•ä¸ªå­—ç¬¦ï¼Œç„¶åé€šè¿‡ç©ºæ ¼æŠŠä»–ä»¬è¿æ¥èµ·æ¥
#             if cleaned_content in content_hashes:
#                 potential_duplicates += 1
#             else:
#                 content_hashes[cleaned_content] = 1
        
#         # ç”ŸæˆæŠ¥å‘Š
#         report = f"## å‘é‡åº“å†…å®¹åˆ†ææŠ¥å‘Š\n\n"
#         report += f"**åŸºæœ¬ä¿¡æ¯ï¼š**\n"
#         report += f"- æ€»æ–‡æ¡£å—æ•°: {total_chunks}\n"
#         report += f"- æ€»å­—ç¬¦æ•°: {total_chars:,}\n"
#         report += f"- å¹³å‡å—é•¿åº¦: {avg_chunk_length:.1f} å­—ç¬¦\n"
#         report += f"- æ½œåœ¨é‡å¤å—: {potential_duplicates}\n\n"
        
#         if source_stats:
#             report += f"**æŒ‰æ¥æºæ–‡ä»¶ç»Ÿè®¡ï¼š**\n"
#             for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
#                 report += f"- {os.path.basename(source)}: {count} å—\n"
        
#         if potential_duplicates > 0:
#             report += f"\n**æ³¨æ„ï¼š** å‘ç° {potential_duplicates} ä¸ªæ½œåœ¨é‡å¤å†…å®¹ï¼Œå»ºè®®è¿è¡Œé‡å¤å†…å®¹æ¸…ç†ã€‚"
        
#         return report
        
#     except Exception as e:
#         error_msg = f"åˆ†æå‘é‡åº“å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {e}"
#         print(error_msg)
#         import traceback
#         traceback.print_exc()
#         return error_msg


# def clear_vector_db():
#     """
#     æ¸…ç©ºæ•´ä¸ªå‘é‡åº“
#     """
#     global vector_db, rag_chain
    
#     if vector_db is None:
#         return "é”™è¯¯ï¼šå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–"
    
#     try:
#         # è·å–æ‰€æœ‰æ–‡æ¡£ID
#         all_docs = vector_db.get()
#         if not all_docs or not all_docs['ids']:
#             return "å‘é‡åº“å·²ç»æ˜¯ç©ºçš„"
        
#         ids_to_delete = all_docs['ids']
#         print(f"å¼€å§‹æ¸…ç©ºå‘é‡åº“ï¼Œåˆ é™¤ {len(ids_to_delete)} ä¸ªæ–‡æ¡£...")
        
#         # åˆ é™¤æ‰€æœ‰æ–‡æ¡£
#         vector_db.delete(ids=ids_to_delete)
        
#         # é‡ç½®RAGé“¾
#         rag_chain = None
        
#         return f"å‘é‡åº“å·²æ¸…ç©ºï¼Œåˆ é™¤äº† {len(ids_to_delete)} ä¸ªæ–‡æ¡£å—"
        
#     except Exception as e:
#         error_msg = f"æ¸…ç©ºå‘é‡åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}"
#         print(error_msg)
#         import traceback
#         traceback.print_exc()
#         return error_msg 